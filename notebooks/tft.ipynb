{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taipei YouBike Forecasting with the Temporal Fusion Transformer (TFT)\n",
    "\n",
    "This notebook adapts the standard TFT workflow for the specific task of forecasting the `occupancy_ratio` of Taipei's YouBike stations. It uses the feature-engineered dataset we prepared previously, which includes time-based features, holiday information, station clusters, and weather data.\n",
    "\n",
    "The key steps are:\n",
    "1.  **Load the pre-processed data.**\n",
    "2.  **Create the `TimeSeriesDataSet`**, which is a special data loader from `pytorch-forecasting` that handles the creation of sequences (the \"sliding window\" we discussed).\n",
    "3.  **Configure and train the TFT model** using PyTorch Lightning.\n",
    "4.  **Evaluate the model** by visualizing its predictions against the actual data.\n",
    "5.  **Interpret the model** to see which features it found most important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer, QuantileLoss\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "import torch\n",
    "\n",
    "# Set a seed for reproducibility\n",
    "pl.seed_everything(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the Pre-processed Data\n",
    "\n",
    "We load the `model_ready_dl_features.parquet.gz` file created in the previous step. This file contains all our engineered features, with categorical variables one-hot encoded and numerical variables normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/content/drive/MyDrive/Youbike_Master_Project/YouBike_Demand_Forecast/data/model_ready_dl_features.parquet.gz\"\n",
    "data = pd.read_parquet(file_path)\n",
    "\n",
    "# Pytorch Forecasting requires a specific data type for the target variable if using a normalizer\n",
    "data['occupancy_ratio'] = data['occupancy_ratio'].astype(np.float32)\n",
    "\n",
    "print(\"Data loaded successfully.\")\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create a Time Index\n",
    "\n",
    "The `pytorch-forecasting` library requires a continuous integer index representing the time steps. We create this by sorting our data and then assigning a cumulative count for each station."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sort_values(['sno', 'time'], inplace=True)\n",
    "data['time_idx'] = data.groupby('sno').cumcount()\n",
    "\n",
    "print(\"Time index created.\")\n",
    "print(data[['sno', 'time', 'time_idx']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Define Model Parameters and Create the `TimeSeriesDataSet`\n",
    "\n",
    "This is the most critical step. We define the sliding window parameters and configure the `TimeSeriesDataSet`, which will handle the memory-efficient creation of training sequences.\n",
    "\n",
    "-   `max_encoder_length`: How many past time steps the model uses for its prediction (the history). 24 hours = 144 steps (24 * 6).\n",
    "-   `max_prediction_length`: How many future time steps the model will forecast. 6 hours = 36 steps (6 * 6).\n",
    "-   **Feature Types:** We must tell the model which features are which:\n",
    "    -   `target`: The variable we want to predict (`occupancy_ratio`).\n",
    "    -   `group_ids`: The identifier for each time series (`sno`).\n",
    "    -   `static_features`: Features that are constant for each station (like its cluster ID).\n",
    "    -   `time_varying_known_reals`: Features we know in advance for the future (e.g., we know the hour of the day and day of the week for tomorrow).\n",
    "    -   `time_varying_unknown_reals`: Features we don't know in advance (e.g., the weather tomorrow, or the occupancy ratio itself)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sliding window parameters\n",
    "max_prediction_length = 6 * 6 # Predict 6 hours ahead\n",
    "max_encoder_length = 24 * 6 # Use 24 hours of history\n",
    "\n",
    "# Define the training/validation split point\n",
    "# We'll use the last month of data for validation\n",
    "training_cutoff = data[\"time_idx\"].max() - (30 * 24 * 6) # 30 days of 10-min intervals\n",
    "\n",
    "# Define all feature columns\n",
    "static_features = [col for col in data.columns if 'cluster_is_' in col]\n",
    "time_varying_known_features = [col for col in data.columns if 'day_is_' in col] + ['hour', 'day_of_week']\n",
    "time_varying_unknown_features = ['occupancy_ratio', 'Temperature', 'Dew Point', 'Humidity', 'Speed', 'Pressure']\n",
    "\n",
    "# Create the TimeSeriesDataSet for training\n",
    "training = TimeSeriesDataSet(\n",
    "    data[lambda x: x.time_idx <= training_cutoff],\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"occupancy_ratio\",\n",
    "    group_ids=[\"sno\"],\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    static_categoricals=static_features,\n",
    "    time_varying_known_reals=time_varying_known_features,\n",
    "    time_varying_unknown_reals=time_varying_unknown_features,\n",
    "    target_normalizer=GroupNormalizer(\n",
    "        groups=[\"sno\"], transformation=\"softplus\"\n",
    "    ),  # Use softplus to ensure predictions are non-negative\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    ")\n",
    "\n",
    "# Create the TimeSeriesDataSet for validation\n",
    "validation = TimeSeriesDataSet.from_dataset(training, data, predict=True, stop_randomization=True)\n",
    "\n",
    "# Create dataloaders for model training\n",
    "batch_size = 128\n",
    "train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=2)\n",
    "val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size * 10, num_workers=2)\n",
    "\n",
    "print(\"TimeSeriesDataSets and DataLoaders created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Configure and Train the TFT Model\n",
    "\n",
    "Now we set up the model using PyTorch Lightning. \n",
    "- We use `EarlyStopping` to monitor the validation loss and stop training automatically when the model stops improving, which prevents overfitting.\n",
    "- A `TensorBoardLogger` is used to log the training progress, which can be visualized later.\n",
    "- The `TemporalFusionTransformer.from_dataset()` method conveniently creates a model with parameters tailored to our specific dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the trainer\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=10, verbose=False, mode=\"min\")\n",
    "logger = TensorBoardLogger(\"lightning_logs\")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=50,\n",
    "    accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "    devices=1,\n",
    "    enable_model_summary=True,\n",
    "    gradient_clip_val=0.1,\n",
    "    callbacks=[early_stop_callback],\n",
    "    logger=logger,\n",
    ")\n",
    "\n",
    "# Define the TFT model\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    learning_rate=0.03,\n",
    "    hidden_size=32,\n",
    "    attention_head_size=4,\n",
    "    dropout=0.1,\n",
    "    hidden_continuous_size=16,\n",
    "    output_size=7,  # 7 quantiles by default\n",
    "    loss=QuantileLoss(),\n",
    "    log_interval=10, \n",
    "    reduce_on_plateau_patience=4,\n",
    ")\n",
    "\n",
    "print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")\n",
    "\n",
    "# Start training\n",
    "# This step can take a very long time depending on your hardware.\n",
    "print(\"Starting model training...\")\n",
    "trainer.fit(\n",
    "    tft,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Evaluate and Visualize Predictions\n",
    "\n",
    "After training, we load the best model (based on the lowest validation loss) and use it to make predictions on the validation set. Visualizing these predictions is the best way to see how well the model has learned the patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model from the checkpoint\n",
    "best_model_path = trainer.checkpoint_callback.best_model_path\n",
    "best_tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "raw_predictions, x = best_tft.predict(val_dataloader, mode=\"raw\", return_x=True)\n",
    "\n",
    "# Visualize some examples\n",
    "print(\"Plotting predictions for 5 random examples from the validation set...\")\n",
    "for i in range(5):\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    best_tft.plot_prediction(x, raw_predictions, idx=i, ax=ax, add_loss_to_title=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Interpret Model Behavior\n",
    "\n",
    "A key advantage of the TFT is its interpretability. We can look at the built-in feature importance plots to understand what the model learned.\n",
    "\n",
    "-   **Encoder Importance:** Which past features were most important for making a prediction.\n",
    "-   **Decoder Importance:** Which future-known features were most important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and plot interpretation\n",
    "interpretation = best_tft.interpret_output(raw_predictions, reduction=\"sum\")\n",
    "figs = best_tft.plot_interpretation(interpretation)\n",
    "\n",
    "# Display the feature importance plots\n",
    "for key, value in figs.items():\n",
    "    print(f\"--- Importance for: {key} ---\")\n",
    "    value.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
