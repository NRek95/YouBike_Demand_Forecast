{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gongguan YouBike Forecasting with the Temporal Fusion Transformer (TFT)\n",
    "\n",
    "This notebook trains a TFT model on the focused **Gongguan case study** dataset. It uses the final feature-engineered data, which includes:\n",
    "- Cyclical time features (sin/cos)\n",
    "- Holiday and storm event flags\n",
    "- Weather data (normalized)\n",
    "- Static POI features (normalized)\n",
    "- Behavioral station clusters (one-hot encoded)\n",
    "- Historical lag and inflow/outflow features (normalized)\n",
    "\n",
    "The workflow is structured to load this data, configure the powerful `TimeSeriesDataSet` loader, train the model, and evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer, QuantileLoss\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pl.seed_everything(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the Pre-processed Gongguan Dataset\n",
    "\n",
    "We load the `gongguan_model_ready_features.parquet.gz` file. This is the final output of our master feature engineering pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/content/drive/MyDrive/Youbike_Master_Project/YouBike_Demand_Forecast/data/gongguan_model_ready_features.parquet.gz\"\n",
    "data = pd.read_parquet(file_path)\n",
    "\n",
    "# Ensure data types are correct for the library\n",
    "data['occupancy_ratio'] = data['occupancy_ratio'].astype(np.float32)\n",
    "# Convert boolean one-hot columns to integers\n",
    "for col in data.columns:\n",
    "    if data[col].dtype == 'bool':\n",
    "        data[col] = data[col].astype(int)\n",
    "\n",
    "print(\"Gongguan dataset loaded successfully.\")\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create a Time Index\n",
    "\n",
    "The `pytorch-forecasting` library requires a continuous integer index for time. We create this by sorting and assigning a cumulative count for each station."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sort_values(['sno', 'time'], inplace=True)\n",
    "# The time index must be an integer\n",
    "data['time_idx'] = data.groupby('sno').cumcount()\n",
    "\n",
    "print(\"Time index created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Define Features and Create the `TimeSeriesDataSet`\n",
    "\n",
    "This is the most critical step. We programmatically identify all our engineered features and assign them to the correct category for the TFT model.\n",
    "\n",
    "-   `max_encoder_length`: How much history to use (e.g., 24 hours).\n",
    "-   `max_prediction_length`: How far to predict into the future (e.g., 6 hours)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sliding window parameters\n",
    "max_prediction_length = 6 * 6 # Predict 6 hours ahead (36 steps)\n",
    "max_encoder_length = 24 * 6 # Use 24 hours of history (144 steps)\n",
    "\n",
    "# Define the training/validation split point (e.g., use last month for validation)\n",
    "training_cutoff = data[\"time_idx\"].max() - (30 * 24 * 6)\n",
    "\n",
    "# --- Automatically identify feature columns ---\n",
    "target = 'occupancy_ratio'\n",
    "group_ids = ['sno']\n",
    "\n",
    "# Static features do not change over time for a given station\n",
    "static_reals = [col for col in data.columns if col.startswith('poi_') or col in ['lat', 'lng']]\n",
    "static_categoricals = [col for col in data.columns if col.startswith('cluster_is_')]\n",
    "\n",
    "# Time-varying features that are known in the future\n",
    "time_varying_known_reals = [\n",
    "    'hour_sin', 'hour_cos', 'day_of_week_sin', 'day_of_week_cos', 'month_sin', 'month_cos',\n",
    "    'is_major_storm_day'\n",
    "] + [col for col in data.columns if col.startswith('day_is_')]\n",
    "\n",
    "# Time-varying features that are not known in the future\n",
    "time_varying_unknown_reals = [\n",
    "    target,\n",
    "    'Temperature', 'Dew Point', 'Humidity', 'Speed', 'Pressure',\n",
    "] + [col for col in data.columns if '_lag_' in col or 'inflow' in col or 'outflow' in col]\n",
    "# Remove the target from the list as it's handled separately\n",
    "time_varying_unknown_reals.remove(target)\n",
    "\n",
    "# --- Create the TimeSeriesDataSet ---\n",
    "training = TimeSeriesDataSet(\n",
    "    data[lambda x: x.time_idx <= training_cutoff],\n",
    "    time_idx=\"time_idx\",\n",
    "    target=target,\n",
    "    group_ids=group_ids,\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    static_categoricals=static_categoricals,\n",
    "    static_reals=static_reals,\n",
    "    time_varying_known_reals=time_varying_known_reals,\n",
    "    time_varying_unknown_reals=time_varying_unknown_reals,\n",
    "    target_normalizer=GroupNormalizer(groups=group_ids, transformation=\"softplus\"),\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    ")\n",
    "\n",
    "validation = TimeSeriesDataSet.from_dataset(training, data, predict=True, stop_randomization=True)\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 128\n",
    "train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=2)\n",
    "val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size * 10, num_workers=2)\n",
    "\n",
    "print(\"TimeSeriesDataSets and DataLoaders created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Configure and Train the TFT Model\n",
    "\n",
    "With the data prepared, we can now define and train our model. We use `EarlyStopping` to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the trainer\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=5, verbose=False, mode=\"min\")\n",
    "logger = TensorBoardLogger(\"lightning_logs\")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=30,\n",
    "    accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "    devices=1,\n",
    "    enable_model_summary=True,\n",
    "    gradient_clip_val=0.1,\n",
    "    callbacks=[early_stop_callback],\n",
    "    logger=logger,\n",
    ")\n",
    "\n",
    "# Define the TFT model\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    learning_rate=0.03,\n",
    "    hidden_size=64, # Increased hidden size for more complex patterns\n",
    "    attention_head_size=4,\n",
    "    dropout=0.1,\n",
    "    hidden_continuous_size=32, # Increased for more features\n",
    "    output_size=7,  # To predict 7 quantiles\n",
    "    loss=QuantileLoss(),\n",
    "    log_interval=10,\n",
    "    reduce_on_plateau_patience=4,\n",
    ")\n",
    "\n",
    "print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")\n",
    "\n",
    "# Start training\n",
    "print(\"Starting model training...\")\n",
    "trainer.fit(\n",
    "    tft,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Evaluate and Visualize Predictions\n",
    "\n",
    "After training, we load the best performing model and visualize its predictions on the validation set to qualitatively assess its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model from the checkpoint\n",
    "best_model_path = trainer.checkpoint_callback.best_model_path\n",
    "best_tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "raw_predictions, x = best_tft.predict(val_dataloader, mode=\"raw\", return_x=True)\n",
    "\n",
    "# Visualize some examples\n",
    "print(\"Plotting predictions for 5 random examples from the validation set...\")\n",
    "for i in range(5):\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    best_tft.plot_prediction(x, raw_predictions, idx=i, ax=ax, add_loss_to_title=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Interpret Model Behavior\n",
    "\n",
    "Finally, we can use TFT's built-in interpretability to see which features the model found most useful for its predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and plot interpretation\n",
    "interpretation = best_tft.interpret_output(raw_predictions, reduction=\"sum\")\n",
    "figs = best_tft.plot_interpretation(interpretation)\n",
    "\n",
    "# Display the feature importance plots\n",
    "for key, value in figs.items():\n",
    "    print(f\"--- Importance for: {key} ---\")\n",
    "    value.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
