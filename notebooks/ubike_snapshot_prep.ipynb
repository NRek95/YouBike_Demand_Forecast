{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_k1_OBlid-e",
        "outputId": "f243cc2e-3292-4905-e8c0-c4dc8c87fea1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7m2ULOrWhf5Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e9e4c59-71f2-4db5-b701-5f14000996ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Step 1: Consolidating and Cleaning Site Information ---\n",
            "Loaded 391601 records from 261 site files.\n",
            "Created a clean lookup table with 1613 unique stations.\n",
            "\n",
            "--- Step 2: Processing Snapshot Data in Batches ---\n",
            "\n",
            "--- Processing Batch 1/9 ---\n",
            "Loaded 5248387 records from 50 files.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Resampling stations: 100%|██████████| 1424/1424 [00:07<00:00, 200.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed batch saved to '/content/drive/MyDrive/Youbike_Master_Project/YouBike_Demand_Forecast/data_clean/temp_batch_1.csv'\n",
            "\n",
            "--- Processing Batch 2/9 ---\n",
            "Loaded 4886354 records from 50 files.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Resampling stations: 100%|██████████| 1439/1439 [00:08<00:00, 169.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed batch saved to '/content/drive/MyDrive/Youbike_Master_Project/YouBike_Demand_Forecast/data_clean/temp_batch_2.csv'\n",
            "\n",
            "--- Processing Batch 3/9 ---\n",
            "Loaded 5580967 records from 50 files.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Resampling stations: 100%|██████████| 1461/1461 [00:11<00:00, 131.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed batch saved to '/content/drive/MyDrive/Youbike_Master_Project/YouBike_Demand_Forecast/data_clean/temp_batch_3.csv'\n",
            "\n",
            "--- Processing Batch 4/9 ---\n",
            "Loaded 3899791 records from 50 files.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Resampling stations: 100%|██████████| 1498/1498 [00:09<00:00, 155.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed batch saved to '/content/drive/MyDrive/Youbike_Master_Project/YouBike_Demand_Forecast/data_clean/temp_batch_4.csv'\n",
            "\n",
            "--- Processing Batch 5/9 ---\n",
            "Loaded 5562547 records from 50 files.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Resampling stations: 100%|██████████| 1499/1499 [00:13<00:00, 113.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed batch saved to '/content/drive/MyDrive/Youbike_Master_Project/YouBike_Demand_Forecast/data_clean/temp_batch_5.csv'\n",
            "\n",
            "--- Processing Batch 6/9 ---\n",
            "Loaded 5502489 records from 50 files.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Resampling stations: 100%|██████████| 1520/1520 [00:16<00:00, 90.53it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed batch saved to '/content/drive/MyDrive/Youbike_Master_Project/YouBike_Demand_Forecast/data_clean/temp_batch_6.csv'\n",
            "\n",
            "--- Processing Batch 7/9 ---\n",
            "Loaded 5015749 records from 50 files.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Resampling stations: 100%|██████████| 1561/1561 [00:10<00:00, 149.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed batch saved to '/content/drive/MyDrive/Youbike_Master_Project/YouBike_Demand_Forecast/data_clean/temp_batch_7.csv'\n",
            "\n",
            "--- Processing Batch 8/9 ---\n",
            "Loaded 3973600 records from 50 files.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Resampling stations: 100%|██████████| 1584/1584 [00:12<00:00, 122.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed batch saved to '/content/drive/MyDrive/Youbike_Master_Project/YouBike_Demand_Forecast/data_clean/temp_batch_8.csv'\n",
            "\n",
            "--- Processing Batch 9/9 ---\n",
            "Loaded 1467244 records from 16 files.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Resampling stations: 100%|██████████| 1593/1593 [00:05<00:00, 285.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed batch saved to '/content/drive/MyDrive/Youbike_Master_Project/YouBike_Demand_Forecast/data_clean/temp_batch_9.csv'\n",
            "\n",
            "--- Step 3: Consolidating all Processed Batches ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Appending remaining batches: 100%|██████████| 8/8 [19:16<00:00, 144.52s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Consolidation of all batches is complete.\n",
            "\n",
            "Preview of the final consolidated DataFrame (first 5 rows):\n",
            "                  mday        sno  total  available_rent_bikes  \\\n",
            "0  2024-05-04 00:00:00  500101001   28.0                   6.0   \n",
            "1  2024-05-04 00:10:00  500101001   28.0                   6.0   \n",
            "2  2024-05-04 00:20:00  500101001   28.0                   3.0   \n",
            "3  2024-05-04 00:30:00  500101001   28.0                   0.0   \n",
            "4  2024-05-04 00:40:00  500101001   28.0                   1.0   \n",
            "\n",
            "   available_return_bikes                 sna       lat       lng     sareaen  \n",
            "0                    22.0  YouBike2.0_捷運科技大樓站  25.02605  121.5436  Daan Dist.  \n",
            "1                    22.0  YouBike2.0_捷運科技大樓站  25.02605  121.5436  Daan Dist.  \n",
            "2                    25.0  YouBike2.0_捷運科技大樓站  25.02605  121.5436  Daan Dist.  \n",
            "3                    28.0  YouBike2.0_捷運科技大樓站  25.02605  121.5436  Daan Dist.  \n",
            "4                    27.0  YouBike2.0_捷運科技大樓站  25.02605  121.5436  Daan Dist.  \n",
            "\n",
            "Master dataset has been saved to: '/content/drive/MyDrive/Youbike_Master_Project/YouBike_Demand_Forecast/data_clean/consolidated_youbike_data_processed.csv'\n",
            "Temporary batch files have been removed.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "def resample_station_data(df: pd.DataFrame, station_id_col: str, timestamp_col: str, freq: str = '10min') -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Resamples irregular time-series data using a robust and efficient pd.merge_asof method.\n",
        "    \"\"\"\n",
        "    df = df.sort_values(timestamp_col)\n",
        "    all_resampled_dfs = []\n",
        "    for station_id, station_df in tqdm(df.groupby(station_id_col), desc=\"Resampling stations\"):\n",
        "        if station_df.empty:\n",
        "            continue\n",
        "        start_time = station_df[timestamp_col].min().floor(freq)\n",
        "        end_time = station_df[timestamp_col].max().ceil(freq)\n",
        "        time_grid = pd.DataFrame({timestamp_col: pd.date_range(start=start_time, end=end_time, freq=freq)})\n",
        "        resampled_station = pd.merge_asof(\n",
        "            left=time_grid,\n",
        "            right=station_df,\n",
        "            on=timestamp_col,\n",
        "            direction='nearest',\n",
        "            tolerance=pd.Timedelta('15min')\n",
        "        )\n",
        "        resampled_station[station_id_col] = station_id\n",
        "        resampled_station = resampled_station.ffill().bfill()\n",
        "        resampled_station.dropna(subset=[c for c in resampled_station.columns if c not in [timestamp_col]], inplace=True)\n",
        "        all_resampled_dfs.append(resampled_station)\n",
        "    if not all_resampled_dfs:\n",
        "        return pd.DataFrame()\n",
        "    final_df = pd.concat(all_resampled_dfs, ignore_index=True)\n",
        "    return final_df\n",
        "\n",
        "# --- Configuration ---\n",
        "DATA_DIR = '/content/drive/MyDrive/Youbike_Master_Project/YouBike_Demand_Forecast/data/'\n",
        "OUTPUT_DIR = '/content/drive/MyDrive/Youbike_Master_Project/YouBike_Demand_Forecast/data_clean/'\n",
        "BATCH_SIZE = 50\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# --- 1. Process and Clean Site Information (Dimension Table) ---\n",
        "print(\"--- Step 1: Consolidating and Cleaning Site Information ---\")\n",
        "site_files = glob.glob(os.path.join(DATA_DIR, '*_site.csv'))\n",
        "\n",
        "if not site_files:\n",
        "    print(f\"Error: No site files found in '{DATA_DIR}'.\")\n",
        "    sites_info_df = None\n",
        "else:\n",
        "    all_sites_df = pd.concat((pd.read_csv(file) for file in site_files), ignore_index=True)\n",
        "    print(f\"Loaded {len(all_sites_df)} records from {len(site_files)} site files.\")\n",
        "    cols_to_drop_from_sites = ['sarea', 'ar']\n",
        "    all_sites_df = all_sites_df.drop(columns=[col for col in cols_to_drop_from_sites if col in all_sites_df.columns])\n",
        "    sites_info_df = all_sites_df.sort_values('sno').drop_duplicates(subset='sno', keep='last').copy()\n",
        "    print(f\"Created a clean lookup table with {len(sites_info_df)} unique stations.\")\n",
        "\n",
        "# --- 2. Process Snapshot Data in Batches to Conserve Memory ---\n",
        "print(\"\\n--- Step 2: Processing Snapshot Data in Batches ---\")\n",
        "slot_files = sorted(glob.glob(os.path.join(DATA_DIR, '*_slot.csv')))\n",
        "processed_batch_files = []\n",
        "\n",
        "if not slot_files or sites_info_df is None:\n",
        "    print(f\"Error: No snapshot/slot files found or site info is missing. Halting.\")\n",
        "else:\n",
        "    num_batches = int(np.ceil(len(slot_files) / BATCH_SIZE))\n",
        "    for i in range(num_batches):\n",
        "        start_index = i * BATCH_SIZE\n",
        "        end_index = start_index + BATCH_SIZE\n",
        "        batch_files = slot_files[start_index:end_index]\n",
        "\n",
        "        print(f\"\\n--- Processing Batch {i+1}/{num_batches} ---\")\n",
        "\n",
        "        batch_df = pd.concat((pd.read_csv(file) for file in batch_files), ignore_index=True)\n",
        "        print(f\"Loaded {len(batch_df)} records from {len(batch_files)} files.\")\n",
        "\n",
        "        timestamp_col = 'infoTime'\n",
        "        numeric_cols = ['total', 'available_rent_bikes', 'available_return_bikes']\n",
        "\n",
        "        batch_df[timestamp_col] = pd.to_datetime(batch_df[timestamp_col], errors='coerce')\n",
        "        batch_df.dropna(subset=[timestamp_col], inplace=True)\n",
        "\n",
        "        for col in numeric_cols:\n",
        "            if col in batch_df.columns:\n",
        "                batch_df[col] = pd.to_numeric(batch_df[col], errors='coerce')\n",
        "\n",
        "        batch_df.dropna(subset=[c for c in numeric_cols if c in batch_df.columns], inplace=True)\n",
        "\n",
        "        resampled_batch_df = resample_station_data(batch_df, station_id_col='sno', timestamp_col=timestamp_col)\n",
        "\n",
        "        # --- THE FIX IS HERE ---\n",
        "        # Include 'sareaen' as it's a critical clustering/categorical feature.\n",
        "        site_info_to_merge = sites_info_df[['sno', 'sna', 'latitude', 'longitude', 'sareaen']].copy()\n",
        "\n",
        "        # Rename for consistency before merging\n",
        "        site_info_to_merge = site_info_to_merge.rename(columns={'latitude': 'lat', 'longitude': 'lng'})\n",
        "\n",
        "        final_batch_df = pd.merge(resampled_batch_df, site_info_to_merge, on='sno', how='left')\n",
        "\n",
        "        batch_output_path = os.path.join(OUTPUT_DIR, f'temp_batch_{i+1}.csv')\n",
        "        final_batch_df.to_csv(batch_output_path, index=False)\n",
        "        processed_batch_files.append(batch_output_path)\n",
        "        print(f\"Processed batch saved to '{batch_output_path}'\")\n",
        "\n",
        "# --- 3. Consolidate Processed Batches into Final Master File (Memory Efficiently) ---\n",
        "# This part remains the same and will correctly handle the new column.\n",
        "print(\"\\n--- Step 3: Consolidating all Processed Batches ---\")\n",
        "if processed_batch_files:\n",
        "    output_path = os.path.join(OUTPUT_DIR, 'consolidated_youbike_data_processed.csv')\n",
        "\n",
        "    first_batch_df = pd.read_csv(processed_batch_files[0])\n",
        "    first_batch_df = first_batch_df.rename(columns={'infoTime': 'mday'})\n",
        "    first_batch_df['mday'] = pd.to_datetime(first_batch_df['mday'])\n",
        "    first_batch_df.to_csv(output_path, index=False, header=True)\n",
        "\n",
        "    if len(processed_batch_files) > 1:\n",
        "        for file in tqdm(processed_batch_files[1:], desc=\"Appending remaining batches\"):\n",
        "            batch_df = pd.read_csv(file)\n",
        "            batch_df = batch_df.rename(columns={'infoTime': 'mday'})\n",
        "            batch_df.to_csv(output_path, mode='a', index=False, header=False)\n",
        "\n",
        "    print(\"\\nConsolidation of all batches is complete.\")\n",
        "    print(\"\\nPreview of the final consolidated DataFrame (first 5 rows):\")\n",
        "    print(pd.read_csv(output_path, nrows=5))\n",
        "    print(f\"\\nMaster dataset has been saved to: '{output_path}'\")\n",
        "\n",
        "    for file in processed_batch_files:\n",
        "        os.remove(file)\n",
        "    print(\"Temporary batch files have been removed.\")\n",
        "else:\n",
        "    print(\"No batches were processed. Final file not created.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fc1ca7cc",
        "outputId": "d583cd7c-9fdd-40f1-fbb7-db5edeef135c"
      },
      "source": [
        "!pip install --upgrade translators"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting translators\n",
            "  Downloading translators-6.0.1-py3-none-any.whl.metadata (70 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/70.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.6/70.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: httpx>=0.28.1 in /usr/local/lib/python3.12/dist-packages (from translators) (0.28.1)\n",
            "Requirement already satisfied: requests>=2.32.3 in /usr/local/lib/python3.12/dist-packages (from translators) (2.32.4)\n",
            "Collecting niquests>=3.14.0 (from translators)\n",
            "  Downloading niquests-3.15.2-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting exejs>=0.0.4 (from translators)\n",
            "  Downloading exejs-0.0.6-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: lxml>=5.4.0 in /usr/local/lib/python3.12/dist-packages (from translators) (5.4.0)\n",
            "Requirement already satisfied: tqdm>=4.67.1 in /usr/local/lib/python3.12/dist-packages (from translators) (4.67.1)\n",
            "Collecting pathos>=0.3.4 (from translators)\n",
            "  Downloading pathos-0.3.4-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting cloudscraper>=1.2.71 (from translators)\n",
            "  Downloading cloudscraper-1.2.71-py2.py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: cryptography>=42.0.4 in /usr/local/lib/python3.12/dist-packages (from translators) (43.0.3)\n",
            "Requirement already satisfied: pyparsing>=2.4.7 in /usr/local/lib/python3.12/dist-packages (from cloudscraper>=1.2.71->translators) (3.2.3)\n",
            "Requirement already satisfied: requests-toolbelt>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from cloudscraper>=1.2.71->translators) (1.0.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=42.0.4->translators) (1.17.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.28.1->translators) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.28.1->translators) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.28.1->translators) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.28.1->translators) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.28.1->translators) (0.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from niquests>=3.14.0->translators) (3.4.3)\n",
            "Collecting urllib3-future<3,>=2.13.903 (from niquests>=3.14.0->translators)\n",
            "  Downloading urllib3_future-2.13.906-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting wassima<3,>=1.0.1 (from niquests>=3.14.0->translators)\n",
            "  Downloading wassima-2.0.1-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting ppft>=1.7.7 (from pathos>=0.3.4->translators)\n",
            "  Downloading ppft-1.7.7-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting dill>=0.4.0 (from pathos>=0.3.4->translators)\n",
            "  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting pox>=0.3.6 (from pathos>=0.3.4->translators)\n",
            "  Downloading pox-0.3.6-py3-none-any.whl.metadata (8.0 kB)\n",
            "Collecting multiprocess>=0.70.18 (from pathos>=0.3.4->translators)\n",
            "  Downloading multiprocess-0.70.18-py312-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.3->translators) (2.5.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=42.0.4->translators) (2.22)\n",
            "Collecting jh2<6.0.0,>=5.0.3 (from urllib3-future<3,>=2.13.903->niquests>=3.14.0->translators)\n",
            "  Downloading jh2-5.0.9-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
            "Collecting qh3<2.0.0,>=1.5.4 (from urllib3-future<3,>=2.13.903->niquests>=3.14.0->translators)\n",
            "  Downloading qh3-1.5.4-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.28.1->translators) (1.3.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.28.1->translators) (4.15.0)\n",
            "Downloading translators-6.0.1-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.6/59.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cloudscraper-1.2.71-py2.py3-none-any.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.7/99.7 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading exejs-0.0.6-py3-none-any.whl (11 kB)\n",
            "Downloading niquests-3.15.2-py3-none-any.whl (167 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.1/167.1 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pathos-0.3.4-py3-none-any.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.3/82.3 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.4.0-py3-none-any.whl (119 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.7/119.7 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.18-py312-none-any.whl (150 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.3/150.3 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pox-0.3.6-py3-none-any.whl (29 kB)\n",
            "Downloading ppft-1.7.7-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading urllib3_future-2.13.906-py3-none-any.whl (670 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.9/670.9 kB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wassima-2.0.1-py3-none-any.whl (145 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.8/145.8 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jh2-5.0.9-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (393 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.4/393.4 kB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading qh3-1.5.4-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m79.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: wassima, qh3, ppft, pox, jh2, exejs, dill, urllib3-future, multiprocess, pathos, niquests, cloudscraper, translators\n",
            "  Attempting uninstall: dill\n",
            "    Found existing installation: dill 0.3.8\n",
            "    Uninstalling dill-0.3.8:\n",
            "      Successfully uninstalled dill-0.3.8\n",
            "  Attempting uninstall: multiprocess\n",
            "    Found existing installation: multiprocess 0.70.16\n",
            "    Uninstalling multiprocess-0.70.16:\n",
            "      Successfully uninstalled multiprocess-0.70.16\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datasets 4.0.0 requires dill<0.3.9,>=0.3.0, but you have dill 0.4.0 which is incompatible.\n",
            "datasets 4.0.0 requires multiprocess<0.70.17, but you have multiprocess 0.70.18 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed cloudscraper-1.2.71 dill-0.4.0 exejs-0.0.6 jh2-5.0.9 multiprocess-0.70.18 niquests-3.15.2 pathos-0.3.4 pox-0.3.6 ppft-1.7.7 qh3-1.5.4 translators-6.0.1 urllib3-future-2.13.906 wassima-2.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- Configuration ---\n",
        "# Ensure this path matches the output directory from your previous script\n",
        "OUTPUT_DIR = '/content/drive/MyDrive/Youbike_Master_Project/YouBike_Demand_Forecast/data_clean/'\n",
        "\n",
        "# --- Step 3: Consolidate Processed Batches into Final Master File (Memory Efficiently) ---\n",
        "print(\"--- Step 3: Consolidating all Processed Batches ---\")\n",
        "\n",
        "# Find all the temporary batch files created by the previous step\n",
        "processed_batch_files = sorted(glob.glob(os.path.join(OUTPUT_DIR, 'temp_batch_*.csv')))\n",
        "\n",
        "if processed_batch_files:\n",
        "    output_path = os.path.join(OUTPUT_DIR, 'consolidated_youbike_data_processed.csv')\n",
        "\n",
        "    # --- Process and write the first batch with a header ---\n",
        "    print(\"Processing first batch to create final file with header...\")\n",
        "    first_batch_df = pd.read_csv(processed_batch_files[0])\n",
        "    first_batch_df = first_batch_df.rename(columns={'infoTime': 'mday'})\n",
        "    first_batch_df['mday'] = pd.to_datetime(first_batch_df['mday'])\n",
        "    first_batch_df.to_csv(output_path, index=False, header=True)\n",
        "\n",
        "    # --- Append the remaining batches without a header ---\n",
        "    if len(processed_batch_files) > 1:\n",
        "        for file in tqdm(processed_batch_files[1:], desc=\"Appending remaining batches\"):\n",
        "            batch_df = pd.read_csv(file)\n",
        "            batch_df = batch_df.rename(columns={'infoTime': 'mday'})\n",
        "            # No need to convert mday to datetime here, as it's just being written to CSV\n",
        "            batch_df.to_csv(output_path, mode='a', index=False, header=False)\n",
        "\n",
        "    print(\"\\nConsolidation of all batches is complete.\")\n",
        "\n",
        "    print(\"\\nPreview of the final consolidated DataFrame (first 5 rows):\")\n",
        "    # Read just the start of the file for a quick preview\n",
        "    print(pd.read_csv(output_path, nrows=5))\n",
        "\n",
        "    print(f\"\\nMaster dataset has been saved to: '{output_path}'\")\n",
        "    print(\"NOTE: The final file is not globally sorted to prevent memory crashes.\")\n",
        "\n",
        "    # Clean up temporary batch files\n",
        "    for file in processed_batch_files:\n",
        "        os.remove(file)\n",
        "    print(\"Temporary batch files have been removed.\")\n",
        "else:\n",
        "    print(\"No processed batch files (temp_batch_*.csv) were found. Final file not created.\")\n"
      ],
      "metadata": {
        "id": "f-qYUF0TMIuL",
        "outputId": "67454a1b-e43c-431f-ed75-b2cf198155dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Step 3: Consolidating all Processed Batches ---\n",
            "Processing first batch to create final file with header...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Appending remaining batches: 100%|██████████| 8/8 [14:59<00:00, 112.43s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Consolidation of all batches is complete.\n",
            "\n",
            "Preview of the final consolidated DataFrame (first 5 rows):\n",
            "                  mday        sno  total  available_rent_bikes  \\\n",
            "0  2024-05-04 00:00:00  500101001   28.0                   6.0   \n",
            "1  2024-05-04 00:10:00  500101001   28.0                   6.0   \n",
            "2  2024-05-04 00:20:00  500101001   28.0                   3.0   \n",
            "3  2024-05-04 00:30:00  500101001   28.0                   0.0   \n",
            "4  2024-05-04 00:40:00  500101001   28.0                   1.0   \n",
            "\n",
            "   available_return_bikes                 sna       lat       lng  \n",
            "0                    22.0  YouBike2.0_捷運科技大樓站  25.02605  121.5436  \n",
            "1                    22.0  YouBike2.0_捷運科技大樓站  25.02605  121.5436  \n",
            "2                    25.0  YouBike2.0_捷運科技大樓站  25.02605  121.5436  \n",
            "3                    28.0  YouBike2.0_捷運科技大樓站  25.02605  121.5436  \n",
            "4                    27.0  YouBike2.0_捷運科技大樓站  25.02605  121.5436  \n",
            "\n",
            "Master dataset has been saved to: '/content/drive/MyDrive/Youbike_Master_Project/YouBike_Demand_Forecast/data_clean/consolidated_youbike_data_processed.csv'\n",
            "NOTE: The final file is not globally sorted to prevent memory crashes.\n",
            "Temporary batch files have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- Configuration ---\n",
        "RAW_DATA_DIR = '/content/drive/MyDrive/Youbike_Master_Project/YouBike_Demand_Forecast/data/'\n",
        "CLEAN_DATA_DIR = '/content/drive/MyDrive/Youbike_Master_Project/YouBike_Demand_Forecast/data_clean/'\n",
        "PROCESSED_FILE_PATH = os.path.join(CLEAN_DATA_DIR, 'consolidated_youbike_data_processed.csv')\n",
        "CHUNK_SIZE = 1_000_000 # Process 1 million rows at a time\n",
        "\n",
        "print(\"--- Starting Memory-Efficient Data Validation ---\")\n",
        "\n",
        "# --- 1. File Existence Check ---\n",
        "print(f\"\\n[1/7] Checking for final processed file: '{PROCESSED_FILE_PATH}'...\")\n",
        "if not os.path.exists(PROCESSED_FILE_PATH):\n",
        "    print(\"... FAIL: Processed file not found. Please run the consolidation script first.\")\n",
        "    exit()\n",
        "print(\"... PASS: File exists.\")\n",
        "\n",
        "# --- 2. Verify Station Completeness against Raw Data ---\n",
        "print(f\"\\n[2/7] Verifying Station Completeness (this step loads raw site files)...\")\n",
        "try:\n",
        "    site_files = glob.glob(os.path.join(RAW_DATA_DIR, '*_site.csv'))\n",
        "    if not site_files:\n",
        "        raise FileNotFoundError(\"No raw site files found.\")\n",
        "\n",
        "    raw_sites_df = pd.concat((pd.read_csv(file, usecols=['sno']) for file in site_files), ignore_index=True)\n",
        "    unique_stations_raw = set(raw_sites_df['sno'].unique())\n",
        "    print(f\"      Found {len(unique_stations_raw):,} unique stations in raw site files.\")\n",
        "\n",
        "    # We will build the processed stations set chunk by chunk\n",
        "    unique_stations_processed = set()\n",
        "    for chunk in tqdm(pd.read_csv(PROCESSED_FILE_PATH, usecols=['sno'], chunksize=CHUNK_SIZE), desc=\"      Scanning for stations\"):\n",
        "        unique_stations_processed.update(chunk['sno'].unique())\n",
        "\n",
        "    missing_stations = unique_stations_raw - unique_stations_processed\n",
        "\n",
        "    if not missing_stations:\n",
        "        print(\"... PASS: All stations from raw data are present in the final processed file.\")\n",
        "    else:\n",
        "        print(f\"... FAIL: {len(missing_stations)} stations from the raw data are missing in the final file.\")\n",
        "        print(f\"      Missing station IDs (sno): {list(missing_stations)}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"... FAIL: An error occurred: {e}\")\n",
        "\n",
        "# --- Initialize variables for chunk-based validation ---\n",
        "nan_report = pd.Series(dtype=int)\n",
        "merge_check_report = pd.Series(dtype=int)\n",
        "total_incorrect_intervals = 0\n",
        "last_row_of_chunk = None\n",
        "\n",
        "# --- Perform Chunk-Based Validations (Checks 3, 4, 5, 6, 7) ---\n",
        "print(\"\\n[3-7] Performing chunk-based validation on the processed file...\")\n",
        "reader = pd.read_csv(PROCESSED_FILE_PATH, chunksize=CHUNK_SIZE, parse_dates=['mday'])\n",
        "\n",
        "for i, chunk in tqdm(enumerate(reader), desc=\"Validating chunks\"):\n",
        "    # First chunk checks\n",
        "    if i == 0:\n",
        "        # --- 6. Verify Column Schema ---\n",
        "        print(\"\\n[6/7] Verifying final column schema (on first chunk)...\")\n",
        "        final_cols = set(chunk.columns)\n",
        "        expected_cols = {'sno', 'mday', 'total', 'available_rent_bikes', 'available_return_bikes', 'sna', 'lat', 'lng', 'sareaen'}\n",
        "        missing_expected = expected_cols - final_cols\n",
        "        if not missing_expected:\n",
        "            print(\"... PASS: Final column schema is correct.\")\n",
        "        else:\n",
        "            print(f\"... FAIL: Missing expected columns: {missing_expected}\")\n",
        "\n",
        "        # --- 7. Check Data Types ---\n",
        "        print(\"\\n[7/7] Verifying column data types (on first chunk)...\")\n",
        "        print(\"      Data types of final DataFrame:\")\n",
        "        print(chunk.dtypes)\n",
        "\n",
        "    # --- 4. Check for Missing Values (NaNs) ---\n",
        "    nan_report = nan_report.add(chunk.isnull().sum(), fill_value=0)\n",
        "\n",
        "    # --- 5. Validate Successful Merging ---\n",
        "    merge_check_cols = ['sna', 'lat', 'lng', 'sareaen']\n",
        "    merge_check_report = merge_check_report.add(chunk[merge_check_cols].isnull().sum(), fill_value=0)\n",
        "\n",
        "    # --- 3. Check for Correct 10-Minute Resampling Interval ---\n",
        "    chunk = chunk.sort_values(by=['sno', 'mday'])\n",
        "\n",
        "    # Check intervals *within* the chunk\n",
        "    chunk['time_diff'] = chunk.groupby('sno')['mday'].diff()\n",
        "    incorrect_in_chunk = chunk[chunk['time_diff'].notna() & (chunk['time_diff'] != pd.Timedelta('10 minutes'))]\n",
        "    total_incorrect_intervals += len(incorrect_in_chunk)\n",
        "\n",
        "    # Check interval *between* the last chunk and this one\n",
        "    if last_row_of_chunk is not None:\n",
        "        first_row_of_chunk = chunk.iloc[0]\n",
        "        if last_row_of_chunk['sno'] == first_row_of_chunk['sno']:\n",
        "            between_chunk_diff = first_row_of_chunk['mday'] - last_row_of_chunk['mday']\n",
        "            if between_chunk_diff != pd.Timedelta('10 minutes'):\n",
        "                total_incorrect_intervals += 1\n",
        "\n",
        "    last_row_of_chunk = chunk.iloc[-1]\n",
        "\n",
        "# --- Final Reports for Chunk-Based Checks ---\n",
        "print(\"\\n--- Final Validation Reports ---\")\n",
        "\n",
        "# Report for Check 4\n",
        "print(\"\\n[4/7] Final Report: Missing Values (NaNs)...\")\n",
        "nan_in_key_cols = nan_report[nan_report > 0]\n",
        "if nan_in_key_cols.empty:\n",
        "    print(\"... PASS: No missing values found in any column across the entire dataset.\")\n",
        "else:\n",
        "    print(\"... FAIL: Missing values were found in the following columns:\")\n",
        "    print(nan_in_key_cols)\n",
        "\n",
        "# Report for Check 5\n",
        "print(\"\\n[5/7] Final Report: Merge Success...\")\n",
        "if merge_check_report.sum() == 0:\n",
        "    print(\"... PASS: Station name, coordinates, and district were successfully merged for all records.\")\n",
        "else:\n",
        "    print(\"... FAIL: Some records are missing site information, indicating a merge issue.\")\n",
        "    print(merge_check_report[merge_check_report > 0])\n",
        "\n",
        "# Report for Check 3\n",
        "print(\"\\n[3/7] Final Report: Resampling Interval...\")\n",
        "if total_incorrect_intervals == 0:\n",
        "    print(\"... PASS: All time intervals between records for each station are exactly 10 minutes.\")\n",
        "else:\n",
        "    print(f\"... FAIL: Found {total_incorrect_intervals} records with incorrect time intervals.\")\n",
        "\n",
        "print(\"\\n--- Validation Complete ---\")\n",
        "\n"
      ],
      "metadata": {
        "id": "2SYwigEutsdN",
        "outputId": "7a1b4c5e-c1b3-4177-b424-0fccae5f1b3e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Memory-Efficient Data Validation ---\n",
            "\n",
            "[1/7] Checking for final processed file: '/content/drive/MyDrive/Youbike_Master_Project/YouBike_Demand_Forecast/data_clean/consolidated_youbike_data_processed.csv'...\n",
            "... PASS: File exists.\n",
            "\n",
            "[2/7] Verifying Station Completeness (this step loads raw site files)...\n",
            "      Found 1,613 unique stations in raw site files.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      Scanning for stations: 89it [01:13,  1.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "... PASS: All stations from raw data are present in the final processed file.\n",
            "\n",
            "[3-7] Performing chunk-based validation on the processed file...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rValidating chunks: 0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[6/7] Verifying final column schema (on first chunk)...\n",
            "... PASS: Final column schema is correct.\n",
            "\n",
            "[7/7] Verifying column data types (on first chunk)...\n",
            "      Data types of final DataFrame:\n",
            "mday                      datetime64[ns]\n",
            "sno                                int64\n",
            "total                            float64\n",
            "available_rent_bikes             float64\n",
            "available_return_bikes           float64\n",
            "sna                               object\n",
            "lat                              float64\n",
            "lng                              float64\n",
            "sareaen                           object\n",
            "dtype: object\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating chunks: 89it [02:17,  1.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Final Validation Reports ---\n",
            "\n",
            "[4/7] Final Report: Missing Values (NaNs)...\n",
            "... PASS: No missing values found in any column across the entire dataset.\n",
            "\n",
            "[5/7] Final Report: Merge Success...\n",
            "... PASS: Station name, coordinates, and district were successfully merged for all records.\n",
            "\n",
            "[3/7] Final Report: Resampling Interval...\n",
            "... PASS: All time intervals between records for each station are exactly 10 minutes.\n",
            "\n",
            "--- Validation Complete ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}