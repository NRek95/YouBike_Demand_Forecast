{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_k1_OBlid-e",
        "outputId": "f243cc2e-3292-4905-e8c0-c4dc8c87fea1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7m2ULOrWhf5Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e9e4c59-71f2-4db5-b701-5f14000996ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Step 1: Consolidating and Cleaning Site Information ---\n",
            "Loaded 391601 records from 261 site files.\n",
            "Created a clean lookup table with 1613 unique stations.\n",
            "\n",
            "--- Step 2: Processing Snapshot Data in Batches ---\n",
            "\n",
            "--- Processing Batch 1/9 ---\n",
            "Loaded 5248387 records from 50 files.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Resampling stations: 100%|██████████| 1424/1424 [00:07<00:00, 200.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed batch saved to '/content/drive/MyDrive/Youbike_Master_Project/YouBike_Demand_Forecast/data_clean/temp_batch_1.csv'\n",
            "\n",
            "--- Processing Batch 2/9 ---\n",
            "Loaded 4886354 records from 50 files.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Resampling stations: 100%|██████████| 1439/1439 [00:08<00:00, 169.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed batch saved to '/content/drive/MyDrive/Youbike_Master_Project/YouBike_Demand_Forecast/data_clean/temp_batch_2.csv'\n",
            "\n",
            "--- Processing Batch 3/9 ---\n",
            "Loaded 5580967 records from 50 files.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Resampling stations: 100%|██████████| 1461/1461 [00:11<00:00, 131.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed batch saved to '/content/drive/MyDrive/Youbike_Master_Project/YouBike_Demand_Forecast/data_clean/temp_batch_3.csv'\n",
            "\n",
            "--- Processing Batch 4/9 ---\n",
            "Loaded 3899791 records from 50 files.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Resampling stations: 100%|██████████| 1498/1498 [00:09<00:00, 155.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed batch saved to '/content/drive/MyDrive/Youbike_Master_Project/YouBike_Demand_Forecast/data_clean/temp_batch_4.csv'\n",
            "\n",
            "--- Processing Batch 5/9 ---\n",
            "Loaded 5562547 records from 50 files.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Resampling stations: 100%|██████████| 1499/1499 [00:13<00:00, 113.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed batch saved to '/content/drive/MyDrive/Youbike_Master_Project/YouBike_Demand_Forecast/data_clean/temp_batch_5.csv'\n",
            "\n",
            "--- Processing Batch 6/9 ---\n",
            "Loaded 5502489 records from 50 files.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Resampling stations: 100%|██████████| 1520/1520 [00:16<00:00, 90.53it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed batch saved to '/content/drive/MyDrive/Youbike_Master_Project/YouBike_Demand_Forecast/data_clean/temp_batch_6.csv'\n",
            "\n",
            "--- Processing Batch 7/9 ---\n",
            "Loaded 5015749 records from 50 files.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Resampling stations: 100%|██████████| 1561/1561 [00:10<00:00, 149.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed batch saved to '/content/drive/MyDrive/Youbike_Master_Project/YouBike_Demand_Forecast/data_clean/temp_batch_7.csv'\n",
            "\n",
            "--- Processing Batch 8/9 ---\n",
            "Loaded 3973600 records from 50 files.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Resampling stations: 100%|██████████| 1584/1584 [00:12<00:00, 122.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed batch saved to '/content/drive/MyDrive/Youbike_Master_Project/YouBike_Demand_Forecast/data_clean/temp_batch_8.csv'\n",
            "\n",
            "--- Processing Batch 9/9 ---\n",
            "Loaded 1467244 records from 16 files.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Resampling stations: 100%|██████████| 1593/1593 [00:05<00:00, 285.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed batch saved to '/content/drive/MyDrive/Youbike_Master_Project/YouBike_Demand_Forecast/data_clean/temp_batch_9.csv'\n",
            "\n",
            "--- Step 3: Consolidating all Processed Batches ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Appending remaining batches: 100%|██████████| 8/8 [19:16<00:00, 144.52s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Consolidation of all batches is complete.\n",
            "\n",
            "Preview of the final consolidated DataFrame (first 5 rows):\n",
            "                  mday        sno  total  available_rent_bikes  \\\n",
            "0  2024-05-04 00:00:00  500101001   28.0                   6.0   \n",
            "1  2024-05-04 00:10:00  500101001   28.0                   6.0   \n",
            "2  2024-05-04 00:20:00  500101001   28.0                   3.0   \n",
            "3  2024-05-04 00:30:00  500101001   28.0                   0.0   \n",
            "4  2024-05-04 00:40:00  500101001   28.0                   1.0   \n",
            "\n",
            "   available_return_bikes                 sna       lat       lng     sareaen  \n",
            "0                    22.0  YouBike2.0_捷運科技大樓站  25.02605  121.5436  Daan Dist.  \n",
            "1                    22.0  YouBike2.0_捷運科技大樓站  25.02605  121.5436  Daan Dist.  \n",
            "2                    25.0  YouBike2.0_捷運科技大樓站  25.02605  121.5436  Daan Dist.  \n",
            "3                    28.0  YouBike2.0_捷運科技大樓站  25.02605  121.5436  Daan Dist.  \n",
            "4                    27.0  YouBike2.0_捷運科技大樓站  25.02605  121.5436  Daan Dist.  \n",
            "\n",
            "Master dataset has been saved to: '/content/drive/MyDrive/Youbike_Master_Project/YouBike_Demand_Forecast/data_clean/consolidated_youbike_data_processed.csv'\n",
            "Temporary batch files have been removed.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "def resample_station_data(df: pd.DataFrame, station_id_col: str, timestamp_col: str, freq: str = '10min') -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Resamples irregular time-series data using a robust and efficient pd.merge_asof method.\n",
        "    \"\"\"\n",
        "    df = df.sort_values(timestamp_col)\n",
        "    all_resampled_dfs = []\n",
        "    for station_id, station_df in tqdm(df.groupby(station_id_col), desc=\"Resampling stations\"):\n",
        "        if station_df.empty:\n",
        "            continue\n",
        "        start_time = station_df[timestamp_col].min().floor(freq)\n",
        "        end_time = station_df[timestamp_col].max().ceil(freq)\n",
        "        time_grid = pd.DataFrame({timestamp_col: pd.date_range(start=start_time, end=end_time, freq=freq)})\n",
        "        resampled_station = pd.merge_asof(\n",
        "            left=time_grid,\n",
        "            right=station_df,\n",
        "            on=timestamp_col,\n",
        "            direction='nearest',\n",
        "            tolerance=pd.Timedelta('15min')\n",
        "        )\n",
        "        resampled_station[station_id_col] = station_id\n",
        "        resampled_station = resampled_station.ffill().bfill()\n",
        "        resampled_station.dropna(subset=[c for c in resampled_station.columns if c not in [timestamp_col]], inplace=True)\n",
        "        all_resampled_dfs.append(resampled_station)\n",
        "    if not all_resampled_dfs:\n",
        "        return pd.DataFrame()\n",
        "    final_df = pd.concat(all_resampled_dfs, ignore_index=True)\n",
        "    return final_df\n",
        "\n",
        "# --- Configuration ---\n",
        "DATA_DIR = '/content/drive/MyDrive/Youbike_Master_Project/YouBike_Demand_Forecast/data/'\n",
        "OUTPUT_DIR = '/content/drive/MyDrive/Youbike_Master_Project/YouBike_Demand_Forecast/data_clean/'\n",
        "BATCH_SIZE = 50\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# --- 1. Process and Clean Site Information (Dimension Table) ---\n",
        "print(\"--- Step 1: Consolidating and Cleaning Site Information ---\")\n",
        "site_files = glob.glob(os.path.join(DATA_DIR, '*_site.csv'))\n",
        "\n",
        "if not site_files:\n",
        "    print(f\"Error: No site files found in '{DATA_DIR}'.\")\n",
        "    sites_info_df = None\n",
        "else:\n",
        "    all_sites_df = pd.concat((pd.read_csv(file) for file in site_files), ignore_index=True)\n",
        "    print(f\"Loaded {len(all_sites_df)} records from {len(site_files)} site files.\")\n",
        "    cols_to_drop_from_sites = ['sarea', 'ar']\n",
        "    all_sites_df = all_sites_df.drop(columns=[col for col in cols_to_drop_from_sites if col in all_sites_df.columns])\n",
        "    sites_info_df = all_sites_df.sort_values('sno').drop_duplicates(subset='sno', keep='last').copy()\n",
        "    print(f\"Created a clean lookup table with {len(sites_info_df)} unique stations.\")\n",
        "\n",
        "# --- 2. Process Snapshot Data in Batches to Conserve Memory ---\n",
        "print(\"\\n--- Step 2: Processing Snapshot Data in Batches ---\")\n",
        "slot_files = sorted(glob.glob(os.path.join(DATA_DIR, '*_slot.csv')))\n",
        "processed_batch_files = []\n",
        "\n",
        "if not slot_files or sites_info_df is None:\n",
        "    print(f\"Error: No snapshot/slot files found or site info is missing. Halting.\")\n",
        "else:\n",
        "    num_batches = int(np.ceil(len(slot_files) / BATCH_SIZE))\n",
        "    for i in range(num_batches):\n",
        "        start_index = i * BATCH_SIZE\n",
        "        end_index = start_index + BATCH_SIZE\n",
        "        batch_files = slot_files[start_index:end_index]\n",
        "\n",
        "        print(f\"\\n--- Processing Batch {i+1}/{num_batches} ---\")\n",
        "\n",
        "        batch_df = pd.concat((pd.read_csv(file) for file in batch_files), ignore_index=True)\n",
        "        print(f\"Loaded {len(batch_df)} records from {len(batch_files)} files.\")\n",
        "\n",
        "        timestamp_col = 'infoTime'\n",
        "        numeric_cols = ['total', 'available_rent_bikes', 'available_return_bikes']\n",
        "\n",
        "        batch_df[timestamp_col] = pd.to_datetime(batch_df[timestamp_col], errors='coerce')\n",
        "        batch_df.dropna(subset=[timestamp_col], inplace=True)\n",
        "\n",
        "        for col in numeric_cols:\n",
        "            if col in batch_df.columns:\n",
        "                batch_df[col] = pd.to_numeric(batch_df[col], errors='coerce')\n",
        "\n",
        "        batch_df.dropna(subset=[c for c in numeric_cols if c in batch_df.columns], inplace=True)\n",
        "\n",
        "        resampled_batch_df = resample_station_data(batch_df, station_id_col='sno', timestamp_col=timestamp_col)\n",
        "\n",
        "        # --- THE FIX IS HERE ---\n",
        "        # Include 'sareaen' as it's a critical clustering/categorical feature.\n",
        "        site_info_to_merge = sites_info_df[['sno', 'sna', 'latitude', 'longitude', 'sareaen']].copy()\n",
        "\n",
        "        # Rename for consistency before merging\n",
        "        site_info_to_merge = site_info_to_merge.rename(columns={'latitude': 'lat', 'longitude': 'lng'})\n",
        "\n",
        "        final_batch_df = pd.merge(resampled_batch_df, site_info_to_merge, on='sno', how='left')\n",
        "\n",
        "        batch_output_path = os.path.join(OUTPUT_DIR, f'temp_batch_{i+1}.csv')\n",
        "        final_batch_df.to_csv(batch_output_path, index=False)\n",
        "        processed_batch_files.append(batch_output_path)\n",
        "        print(f\"Processed batch saved to '{batch_output_path}'\")\n",
        "\n",
        "# --- 3. Consolidate Processed Batches into Final Master File (Memory Efficiently) ---\n",
        "# This part remains the same and will correctly handle the new column.\n",
        "print(\"\\n--- Step 3: Consolidating all Processed Batches ---\")\n",
        "if processed_batch_files:\n",
        "    output_path = os.path.join(OUTPUT_DIR, 'consolidated_youbike_data_processed.csv')\n",
        "\n",
        "    first_batch_df = pd.read_csv(processed_batch_files[0])\n",
        "    first_batch_df = first_batch_df.rename(columns={'infoTime': 'mday'})\n",
        "    first_batch_df['mday'] = pd.to_datetime(first_batch_df['mday'])\n",
        "    first_batch_df.to_csv(output_path, index=False, header=True)\n",
        "\n",
        "    if len(processed_batch_files) > 1:\n",
        "        for file in tqdm(processed_batch_files[1:], desc=\"Appending remaining batches\"):\n",
        "            batch_df = pd.read_csv(file)\n",
        "            batch_df = batch_df.rename(columns={'infoTime': 'mday'})\n",
        "            batch_df.to_csv(output_path, mode='a', index=False, header=False)\n",
        "\n",
        "    print(\"\\nConsolidation of all batches is complete.\")\n",
        "    print(\"\\nPreview of the final consolidated DataFrame (first 5 rows):\")\n",
        "    print(pd.read_csv(output_path, nrows=5))\n",
        "    print(f\"\\nMaster dataset has been saved to: '{output_path}'\")\n",
        "\n",
        "    for file in processed_batch_files:\n",
        "        os.remove(file)\n",
        "    print(\"Temporary batch files have been removed.\")\n",
        "else:\n",
        "    print(\"No batches were processed. Final file not created.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fc1ca7cc",
        "outputId": "d583cd7c-9fdd-40f1-fbb7-db5edeef135c"
      },
      "source": [
        "!pip install --upgrade translators"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting translators\n",
            "  Downloading translators-6.0.1-py3-none-any.whl.metadata (70 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/70.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.6/70.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: httpx>=0.28.1 in /usr/local/lib/python3.12/dist-packages (from translators) (0.28.1)\n",
            "Requirement already satisfied: requests>=2.32.3 in /usr/local/lib/python3.12/dist-packages (from translators) (2.32.4)\n",
            "Collecting niquests>=3.14.0 (from translators)\n",
            "  Downloading niquests-3.15.2-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting exejs>=0.0.4 (from translators)\n",
            "  Downloading exejs-0.0.6-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: lxml>=5.4.0 in /usr/local/lib/python3.12/dist-packages (from translators) (5.4.0)\n",
            "Requirement already satisfied: tqdm>=4.67.1 in /usr/local/lib/python3.12/dist-packages (from translators) (4.67.1)\n",
            "Collecting pathos>=0.3.4 (from translators)\n",
            "  Downloading pathos-0.3.4-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting cloudscraper>=1.2.71 (from translators)\n",
            "  Downloading cloudscraper-1.2.71-py2.py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: cryptography>=42.0.4 in /usr/local/lib/python3.12/dist-packages (from translators) (43.0.3)\n",
            "Requirement already satisfied: pyparsing>=2.4.7 in /usr/local/lib/python3.12/dist-packages (from cloudscraper>=1.2.71->translators) (3.2.3)\n",
            "Requirement already satisfied: requests-toolbelt>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from cloudscraper>=1.2.71->translators) (1.0.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=42.0.4->translators) (1.17.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.28.1->translators) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.28.1->translators) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.28.1->translators) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.28.1->translators) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.28.1->translators) (0.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from niquests>=3.14.0->translators) (3.4.3)\n",
            "Collecting urllib3-future<3,>=2.13.903 (from niquests>=3.14.0->translators)\n",
            "  Downloading urllib3_future-2.13.906-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting wassima<3,>=1.0.1 (from niquests>=3.14.0->translators)\n",
            "  Downloading wassima-2.0.1-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting ppft>=1.7.7 (from pathos>=0.3.4->translators)\n",
            "  Downloading ppft-1.7.7-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting dill>=0.4.0 (from pathos>=0.3.4->translators)\n",
            "  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting pox>=0.3.6 (from pathos>=0.3.4->translators)\n",
            "  Downloading pox-0.3.6-py3-none-any.whl.metadata (8.0 kB)\n",
            "Collecting multiprocess>=0.70.18 (from pathos>=0.3.4->translators)\n",
            "  Downloading multiprocess-0.70.18-py312-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.3->translators) (2.5.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=42.0.4->translators) (2.22)\n",
            "Collecting jh2<6.0.0,>=5.0.3 (from urllib3-future<3,>=2.13.903->niquests>=3.14.0->translators)\n",
            "  Downloading jh2-5.0.9-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
            "Collecting qh3<2.0.0,>=1.5.4 (from urllib3-future<3,>=2.13.903->niquests>=3.14.0->translators)\n",
            "  Downloading qh3-1.5.4-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.28.1->translators) (1.3.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.28.1->translators) (4.15.0)\n",
            "Downloading translators-6.0.1-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.6/59.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cloudscraper-1.2.71-py2.py3-none-any.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.7/99.7 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading exejs-0.0.6-py3-none-any.whl (11 kB)\n",
            "Downloading niquests-3.15.2-py3-none-any.whl (167 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.1/167.1 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pathos-0.3.4-py3-none-any.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.3/82.3 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.4.0-py3-none-any.whl (119 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.7/119.7 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.18-py312-none-any.whl (150 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.3/150.3 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pox-0.3.6-py3-none-any.whl (29 kB)\n",
            "Downloading ppft-1.7.7-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading urllib3_future-2.13.906-py3-none-any.whl (670 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.9/670.9 kB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wassima-2.0.1-py3-none-any.whl (145 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.8/145.8 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jh2-5.0.9-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (393 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.4/393.4 kB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading qh3-1.5.4-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m79.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: wassima, qh3, ppft, pox, jh2, exejs, dill, urllib3-future, multiprocess, pathos, niquests, cloudscraper, translators\n",
            "  Attempting uninstall: dill\n",
            "    Found existing installation: dill 0.3.8\n",
            "    Uninstalling dill-0.3.8:\n",
            "      Successfully uninstalled dill-0.3.8\n",
            "  Attempting uninstall: multiprocess\n",
            "    Found existing installation: multiprocess 0.70.16\n",
            "    Uninstalling multiprocess-0.70.16:\n",
            "      Successfully uninstalled multiprocess-0.70.16\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datasets 4.0.0 requires dill<0.3.9,>=0.3.0, but you have dill 0.4.0 which is incompatible.\n",
            "datasets 4.0.0 requires multiprocess<0.70.17, but you have multiprocess 0.70.18 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed cloudscraper-1.2.71 dill-0.4.0 exejs-0.0.6 jh2-5.0.9 multiprocess-0.70.18 niquests-3.15.2 pathos-0.3.4 pox-0.3.6 ppft-1.7.7 qh3-1.5.4 translators-6.0.1 urllib3-future-2.13.906 wassima-2.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- Configuration ---\n",
        "# Ensure this path matches the output directory from your previous script\n",
        "OUTPUT_DIR = '/content/drive/MyDrive/Youbike_Master_Project/YouBike_Demand_Forecast/data_clean/'\n",
        "\n",
        "# --- Step 3: Consolidate Processed Batches into Final Master File (Memory Efficiently) ---\n",
        "print(\"--- Step 3: Consolidating all Processed Batches ---\")\n",
        "\n",
        "# Find all the temporary batch files created by the previous step\n",
        "processed_batch_files = sorted(glob.glob(os.path.join(OUTPUT_DIR, 'temp_batch_*.csv')))\n",
        "\n",
        "if processed_batch_files:\n",
        "    output_path = os.path.join(OUTPUT_DIR, 'consolidated_youbike_data_processed.csv')\n",
        "\n",
        "    # --- Process and write the first batch with a header ---\n",
        "    print(\"Processing first batch to create final file with header...\")\n",
        "    first_batch_df = pd.read_csv(processed_batch_files[0])\n",
        "    first_batch_df = first_batch_df.rename(columns={'infoTime': 'mday'})\n",
        "    first_batch_df['mday'] = pd.to_datetime(first_batch_df['mday'])\n",
        "    first_batch_df.to_csv(output_path, index=False, header=True)\n",
        "\n",
        "    # --- Append the remaining batches without a header ---\n",
        "    if len(processed_batch_files) > 1:\n",
        "        for file in tqdm(processed_batch_files[1:], desc=\"Appending remaining batches\"):\n",
        "            batch_df = pd.read_csv(file)\n",
        "            batch_df = batch_df.rename(columns={'infoTime': 'mday'})\n",
        "            # No need to convert mday to datetime here, as it's just being written to CSV\n",
        "            batch_df.to_csv(output_path, mode='a', index=False, header=False)\n",
        "\n",
        "    print(\"\\nConsolidation of all batches is complete.\")\n",
        "\n",
        "    print(\"\\nPreview of the final consolidated DataFrame (first 5 rows):\")\n",
        "    # Read just the start of the file for a quick preview\n",
        "    print(pd.read_csv(output_path, nrows=5))\n",
        "\n",
        "    print(f\"\\nMaster dataset has been saved to: '{output_path}'\")\n",
        "    print(\"NOTE: The final file is not globally sorted to prevent memory crashes.\")\n",
        "\n",
        "    # Clean up temporary batch files\n",
        "    for file in processed_batch_files:\n",
        "        os.remove(file)\n",
        "    print(\"Temporary batch files have been removed.\")\n",
        "else:\n",
        "    print(\"No processed batch files (temp_batch_*.csv) were found. Final file not created.\")\n"
      ],
      "metadata": {
        "id": "f-qYUF0TMIuL",
        "outputId": "67454a1b-e43c-431f-ed75-b2cf198155dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Step 3: Consolidating all Processed Batches ---\n",
            "Processing first batch to create final file with header...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Appending remaining batches: 100%|██████████| 8/8 [14:59<00:00, 112.43s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Consolidation of all batches is complete.\n",
            "\n",
            "Preview of the final consolidated DataFrame (first 5 rows):\n",
            "                  mday        sno  total  available_rent_bikes  \\\n",
            "0  2024-05-04 00:00:00  500101001   28.0                   6.0   \n",
            "1  2024-05-04 00:10:00  500101001   28.0                   6.0   \n",
            "2  2024-05-04 00:20:00  500101001   28.0                   3.0   \n",
            "3  2024-05-04 00:30:00  500101001   28.0                   0.0   \n",
            "4  2024-05-04 00:40:00  500101001   28.0                   1.0   \n",
            "\n",
            "   available_return_bikes                 sna       lat       lng  \n",
            "0                    22.0  YouBike2.0_捷運科技大樓站  25.02605  121.5436  \n",
            "1                    22.0  YouBike2.0_捷運科技大樓站  25.02605  121.5436  \n",
            "2                    25.0  YouBike2.0_捷運科技大樓站  25.02605  121.5436  \n",
            "3                    28.0  YouBike2.0_捷運科技大樓站  25.02605  121.5436  \n",
            "4                    27.0  YouBike2.0_捷運科技大樓站  25.02605  121.5436  \n",
            "\n",
            "Master dataset has been saved to: '/content/drive/MyDrive/Youbike_Master_Project/YouBike_Demand_Forecast/data_clean/consolidated_youbike_data_processed.csv'\n",
            "NOTE: The final file is not globally sorted to prevent memory crashes.\n",
            "Temporary batch files have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- Configuration ---\n",
        "RAW_DATA_DIR = '/content/drive/MyDrive/Youbike_Master_Project/YouBike_Demand_Forecast/data/'\n",
        "CLEAN_DATA_DIR = '/content/drive/MyDrive/Youbike_Master_Project/YouBike_Demand_Forecast/data_clean/'\n",
        "PROCESSED_FILE_PATH = os.path.join(CLEAN_DATA_DIR, 'consolidated_youbike_data_processed.csv')\n",
        "CHUNK_SIZE = 1_000_000 # Process 1 million rows at a time\n",
        "\n",
        "print(\"--- Starting Memory-Efficient Data Validation ---\")\n",
        "\n",
        "# --- 1. File Existence Check ---\n",
        "print(f\"\\n[1/7] Checking for final processed file: '{PROCESSED_FILE_PATH}'...\")\n",
        "if not os.path.exists(PROCESSED_FILE_PATH):\n",
        "    print(\"... FAIL: Processed file not found. Please run the consolidation script first.\")\n",
        "    exit()\n",
        "print(\"... PASS: File exists.\")\n",
        "\n",
        "# --- 2. Verify Station Completeness against Raw Data ---\n",
        "print(f\"\\n[2/7] Verifying Station Completeness (this step loads raw site files)...\")\n",
        "try:\n",
        "    site_files = glob.glob(os.path.join(RAW_DATA_DIR, '*_site.csv'))\n",
        "    if not site_files:\n",
        "        raise FileNotFoundError(\"No raw site files found.\")\n",
        "\n",
        "    raw_sites_df = pd.concat((pd.read_csv(file, usecols=['sno']) for file in site_files), ignore_index=True)\n",
        "    unique_stations_raw = set(raw_sites_df['sno'].unique())\n",
        "    print(f\"      Found {len(unique_stations_raw):,} unique stations in raw site files.\")\n",
        "\n",
        "    # We will build the processed stations set chunk by chunk\n",
        "    unique_stations_processed = set()\n",
        "    for chunk in tqdm(pd.read_csv(PROCESSED_FILE_PATH, usecols=['sno'], chunksize=CHUNK_SIZE), desc=\"      Scanning for stations\"):\n",
        "        unique_stations_processed.update(chunk['sno'].unique())\n",
        "\n",
        "    missing_stations = unique_stations_raw - unique_stations_processed\n",
        "\n",
        "    if not missing_stations:\n",
        "        print(\"... PASS: All stations from raw data are present in the final processed file.\")\n",
        "    else:\n",
        "        print(f\"... FAIL: {len(missing_stations)} stations from the raw data are missing in the final file.\")\n",
        "        print(f\"      Missing station IDs (sno): {list(missing_stations)}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"... FAIL: An error occurred: {e}\")\n",
        "\n",
        "# --- Initialize variables for chunk-based validation ---\n",
        "nan_report = pd.Series(dtype=int)\n",
        "merge_check_report = pd.Series(dtype=int)\n",
        "total_incorrect_intervals = 0\n",
        "last_row_of_chunk = None\n",
        "\n",
        "# --- Perform Chunk-Based Validations (Checks 3, 4, 5, 6, 7) ---\n",
        "print(\"\\n[3-7] Performing chunk-based validation on the processed file...\")\n",
        "reader = pd.read_csv(PROCESSED_FILE_PATH, chunksize=CHUNK_SIZE, parse_dates=['mday'])\n",
        "\n",
        "for i, chunk in tqdm(enumerate(reader), desc=\"Validating chunks\"):\n",
        "    # First chunk checks\n",
        "    if i == 0:\n",
        "        # --- 6. Verify Column Schema ---\n",
        "        print(\"\\n[6/7] Verifying final column schema (on first chunk)...\")\n",
        "        final_cols = set(chunk.columns)\n",
        "        expected_cols = {'sno', 'mday', 'total', 'available_rent_bikes', 'available_return_bikes', 'sna', 'lat', 'lng', 'sareaen'}\n",
        "        missing_expected = expected_cols - final_cols\n",
        "        if not missing_expected:\n",
        "            print(\"... PASS: Final column schema is correct.\")\n",
        "        else:\n",
        "            print(f\"... FAIL: Missing expected columns: {missing_expected}\")\n",
        "\n",
        "        # --- 7. Check Data Types ---\n",
        "        print(\"\\n[7/7] Verifying column data types (on first chunk)...\")\n",
        "        print(\"      Data types of final DataFrame:\")\n",
        "        print(chunk.dtypes)\n",
        "\n",
        "    # --- 4. Check for Missing Values (NaNs) ---\n",
        "    nan_report = nan_report.add(chunk.isnull().sum(), fill_value=0)\n",
        "\n",
        "    # --- 5. Validate Successful Merging ---\n",
        "    merge_check_cols = ['sna', 'lat', 'lng', 'sareaen']\n",
        "    merge_check_report = merge_check_report.add(chunk[merge_check_cols].isnull().sum(), fill_value=0)\n",
        "\n",
        "    # --- 3. Check for Correct 10-Minute Resampling Interval ---\n",
        "    chunk = chunk.sort_values(by=['sno', 'mday'])\n",
        "\n",
        "    # Check intervals *within* the chunk\n",
        "    chunk['time_diff'] = chunk.groupby('sno')['mday'].diff()\n",
        "    incorrect_in_chunk = chunk[chunk['time_diff'].notna() & (chunk['time_diff'] != pd.Timedelta('10 minutes'))]\n",
        "    total_incorrect_intervals += len(incorrect_in_chunk)\n",
        "\n",
        "    # Check interval *between* the last chunk and this one\n",
        "    if last_row_of_chunk is not None:\n",
        "        first_row_of_chunk = chunk.iloc[0]\n",
        "        if last_row_of_chunk['sno'] == first_row_of_chunk['sno']:\n",
        "            between_chunk_diff = first_row_of_chunk['mday'] - last_row_of_chunk['mday']\n",
        "            if between_chunk_diff != pd.Timedelta('10 minutes'):\n",
        "                total_incorrect_intervals += 1\n",
        "\n",
        "    last_row_of_chunk = chunk.iloc[-1]\n",
        "\n",
        "# --- Final Reports for Chunk-Based Checks ---\n",
        "print(\"\\n--- Final Validation Reports ---\")\n",
        "\n",
        "# Report for Check 4\n",
        "print(\"\\n[4/7] Final Report: Missing Values (NaNs)...\")\n",
        "nan_in_key_cols = nan_report[nan_report > 0]\n",
        "if nan_in_key_cols.empty:\n",
        "    print(\"... PASS: No missing values found in any column across the entire dataset.\")\n",
        "else:\n",
        "    print(\"... FAIL: Missing values were found in the following columns:\")\n",
        "    print(nan_in_key_cols)\n",
        "\n",
        "# Report for Check 5\n",
        "print(\"\\n[5/7] Final Report: Merge Success...\")\n",
        "if merge_check_report.sum() == 0:\n",
        "    print(\"... PASS: Station name, coordinates, and district were successfully merged for all records.\")\n",
        "else:\n",
        "    print(\"... FAIL: Some records are missing site information, indicating a merge issue.\")\n",
        "    print(merge_check_report[merge_check_report > 0])\n",
        "\n",
        "# Report for Check 3\n",
        "print(\"\\n[3/7] Final Report: Resampling Interval...\")\n",
        "if total_incorrect_intervals == 0:\n",
        "    print(\"... PASS: All time intervals between records for each station are exactly 10 minutes.\")\n",
        "else:\n",
        "    print(f\"... FAIL: Found {total_incorrect_intervals} records with incorrect time intervals.\")\n",
        "\n",
        "print(\"\\n--- Validation Complete ---\")\n",
        "\n"
      ],
      "metadata": {
        "id": "2SYwigEutsdN",
        "outputId": "7a1b4c5e-c1b3-4177-b424-0fccae5f1b3e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Memory-Efficient Data Validation ---\n",
            "\n",
            "[1/7] Checking for final processed file: '/content/drive/MyDrive/Youbike_Master_Project/YouBike_Demand_Forecast/data_clean/consolidated_youbike_data_processed.csv'...\n",
            "... PASS: File exists.\n",
            "\n",
            "[2/7] Verifying Station Completeness (this step loads raw site files)...\n",
            "      Found 1,613 unique stations in raw site files.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      Scanning for stations: 89it [01:13,  1.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "... PASS: All stations from raw data are present in the final processed file.\n",
            "\n",
            "[3-7] Performing chunk-based validation on the processed file...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rValidating chunks: 0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[6/7] Verifying final column schema (on first chunk)...\n",
            "... PASS: Final column schema is correct.\n",
            "\n",
            "[7/7] Verifying column data types (on first chunk)...\n",
            "      Data types of final DataFrame:\n",
            "mday                      datetime64[ns]\n",
            "sno                                int64\n",
            "total                            float64\n",
            "available_rent_bikes             float64\n",
            "available_return_bikes           float64\n",
            "sna                               object\n",
            "lat                              float64\n",
            "lng                              float64\n",
            "sareaen                           object\n",
            "dtype: object\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating chunks: 89it [02:17,  1.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Final Validation Reports ---\n",
            "\n",
            "[4/7] Final Report: Missing Values (NaNs)...\n",
            "... PASS: No missing values found in any column across the entire dataset.\n",
            "\n",
            "[5/7] Final Report: Merge Success...\n",
            "... PASS: Station name, coordinates, and district were successfully merged for all records.\n",
            "\n",
            "[3/7] Final Report: Resampling Interval...\n",
            "... PASS: All time intervals between records for each station are exactly 10 minutes.\n",
            "\n",
            "--- Validation Complete ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the file path\n",
        "FILE_PATH = '/content/drive/MyDrive/Youbike_Master_Project/YouBike_Demand_Forecast/data_clean/consolidated_youbike_data_processed.csv'\n",
        "\n",
        "try:\n",
        "    # Use the 'usecols' parameter to load only the 'sareaen' column\n",
        "    print(\"Reading only the 'sareaen' column to save memory...\")\n",
        "    df_column = pd.read_csv(FILE_PATH, usecols=['sareaen'])\n",
        "\n",
        "    # Get the unique values from that single column\n",
        "    unique_districts = df_column['sareaen'].unique()\n",
        "\n",
        "    print(\"\\n✅ Success! Here are the unique districts found in your file:\")\n",
        "    for district in sorted(unique_districts): # sorted() makes the list alphabetical\n",
        "        print(district)\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"\\n❌ Error: The file was not found at '{FILE_PATH}'.\")\n",
        "    print(\"Please double-check the path, folder names, and file name are correct.\")\n",
        "except ValueError as e:\n",
        "    # This error often occurs if 'sareaen' is not in the CSV's header\n",
        "    print(f\"\\n❌ Error: {e}. Is 'sareaen' definitely the correct column name?\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQL1MEkQ9-EE",
        "outputId": "7ac25b87-5b31-4c2f-f1c6-7a19e08e696f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Reading only the 'sareaen' column to save memory...\n",
            "\n",
            "✅ Success! Here are the unique districts found in your file:\n",
            "Beitou Dist\n",
            "Daan Dist.\n",
            "Datong Dist\n",
            "NTU Dist\n",
            "Nangang Dist\n",
            "Neihu Dist\n",
            "Shilin Dist\n",
            "Songshan Dist\n",
            "Wanhua Dist\n",
            "Wenshan Dist\n",
            "Xinyi Dist\n",
            "Zhongshan Dist\n",
            "Zhongzheng Dist\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# --- 1. Define Paths and Mount Drive ---\n",
        "print(\"Connecting to Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# The specific folder you want to save the file in\n",
        "CLEAN_DATA_DIR = '/content/drive/MyDrive/Youbike_Master_Project/YouBike_Demand_Forecast/data_clean/'\n",
        "# The name for your new file\n",
        "output_filename = 'youbike_station_names.csv'\n",
        "# Combine the folder and filename into a full path\n",
        "full_output_path = os.path.join(CLEAN_DATA_DIR, output_filename)\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "os.makedirs(CLEAN_DATA_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "# --- 2. Fetch and Process Data ---\n",
        "url = \"https://tcgbusfs.blob.core.windows.net/dotapp/youbike/v2/youbike_immediate.json\"\n",
        "\n",
        "try:\n",
        "    print(\"\\nFetching and processing data...\")\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()\n",
        "    data = response.json()\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # Clean the 'snaen' column by removing the prefix\n",
        "    df['snaen'] = df['snaen'].str.removeprefix('YouBike2.0_')\n",
        "\n",
        "    # Select only the 'sno' and the now-cleaned 'snaen' columns\n",
        "    df_to_save = df[['sno', 'snaen']]\n",
        "    print(\"✓ Data processed and filtered successfully.\")\n",
        "\n",
        "    print(\"\\nPreview of the data to be saved:\")\n",
        "    print(df_to_save.head())\n",
        "\n",
        "\n",
        "    # --- 3. Save the Filtered DataFrame to CSV ---\n",
        "    df_to_save.to_csv(full_output_path, index=False, encoding='utf-8-sig')\n",
        "    print(f\"\\n✅ Success! Data has been saved to:\\n{full_output_path}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3qLxRkkfM0_",
        "outputId": "9bd9e4b4-47a8-449d-a718-bee75243dad8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connecting to Google Drive...\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "\n",
            "Fetching and processing data...\n",
            "✓ Data processed and filtered successfully.\n",
            "\n",
            "Preview of the data to be saved:\n",
            "         sno                                       snaen\n",
            "0  500101001                   MRT Technology Bldg. Sta.\n",
            "1  500101002               No.273， Sec. 2， Fuxing S. Rd.\n",
            "2  500101003    NTUE Experiment Elementary School (East)\n",
            "3  500101004                          Heping Park (East)\n",
            "4  500101005  Xinhai Fuxing Rd. Intersection (Northwest)\n",
            "\n",
            "✅ Success! Data has been saved to:\n",
            "/content/drive/MyDrive/Youbike_Master_Project/YouBike_Demand_Forecast/data_clean/youbike_station_names.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_to_save.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "yijAO3lbhbDZ",
        "outputId": "5528be42-5a94-4418-dc43-12472094c6e0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         sno                                       snaen\n",
              "0  500101001                   MRT Technology Bldg. Sta.\n",
              "1  500101002               No.273， Sec. 2， Fuxing S. Rd.\n",
              "2  500101003    NTUE Experiment Elementary School (East)\n",
              "3  500101004                          Heping Park (East)\n",
              "4  500101005  Xinhai Fuxing Rd. Intersection (Northwest)"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7763c1ec-532f-4783-87ce-34d94b207c12\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sno</th>\n",
              "      <th>snaen</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>500101001</td>\n",
              "      <td>MRT Technology Bldg. Sta.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>500101002</td>\n",
              "      <td>No.273， Sec. 2， Fuxing S. Rd.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>500101003</td>\n",
              "      <td>NTUE Experiment Elementary School (East)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>500101004</td>\n",
              "      <td>Heping Park (East)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>500101005</td>\n",
              "      <td>Xinhai Fuxing Rd. Intersection (Northwest)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7763c1ec-532f-4783-87ce-34d94b207c12')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7763c1ec-532f-4783-87ce-34d94b207c12 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7763c1ec-532f-4783-87ce-34d94b207c12');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-fc303ffe-4e0c-404b-acba-fb288ce6ab51\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-fc303ffe-4e0c-404b-acba-fb288ce6ab51')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-fc303ffe-4e0c-404b-acba-fb288ce6ab51 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_to_save",
              "summary": "{\n  \"name\": \"df_to_save\",\n  \"rows\": 1647,\n  \"fields\": [\n    {\n      \"column\": \"sno\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1647,\n        \"samples\": [\n          \"500107019\",\n          \"500112036\",\n          \"500107064\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"snaen\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1640,\n        \"samples\": [\n          \"Yijiang Park\",\n          \"Ln. 130\\uff0c Sec. 3\\uff0c Nangang Rd.\",\n          \"Primax Headquarters Building\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# --- 1. Setup Paths and Mount Drive ---\n",
        "print(\"Connecting to Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "CLEAN_DATA_DIR = '/content/drive/MyDrive/Youbike_Master_Project/YouBike_Demand_Forecast/data_clean/'\n",
        "input_filename = 'consolidated_youbike_data_processed.csv'\n",
        "output_filename = 'consolidated_data_with_snaen_corrected.csv' # New output file\n",
        "\n",
        "input_path = os.path.join(CLEAN_DATA_DIR, input_filename)\n",
        "output_path = os.path.join(CLEAN_DATA_DIR, output_filename)\n",
        "\n",
        "\n",
        "# --- 2. Fetch Master Station List and Standardize Data Type ---\n",
        "url = \"https://tcgbusfs.blob.core.windows.net/dotapp/youbike/v2/youbike_immediate.json\"\n",
        "print(\"\\nFetching master station list from the YouBike API...\")\n",
        "\n",
        "try:\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()\n",
        "    api_data = response.json()\n",
        "\n",
        "    station_map_df = pd.DataFrame(api_data)\n",
        "\n",
        "    # **FIX 1: Convert API 'sno' to string before creating the lookup**\n",
        "    station_map_df['sno'] = station_map_df['sno'].astype(str)\n",
        "\n",
        "    station_map_df['snaen'] = station_map_df['snaen'].str.removeprefix('YouBike2.0_')\n",
        "\n",
        "    station_lookup = station_map_df.set_index('sno')['snaen'].to_dict()\n",
        "    print(f\"✓ Created a lookup map for {len(station_lookup)} unique stations.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error fetching or processing API data: {e}\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "# --- 3. Process the Large CSV in Chunks with Standardized Data Type ---\n",
        "print(f\"\\nProcessing your large dataset from: {input_path}\")\n",
        "try:\n",
        "    chunk_num = 1\n",
        "\n",
        "    chunk_iterator = pd.read_csv(input_path, chunksize=500000, low_memory=False)\n",
        "\n",
        "    for chunk in chunk_iterator:\n",
        "        print(f\"  -> Processing chunk {chunk_num}...\")\n",
        "\n",
        "        # **FIX 2: Convert 'sno' in your data chunk to string before mapping**\n",
        "        chunk['sno'] = chunk['sno'].astype(str)\n",
        "\n",
        "        # Add the 'snaen' column using the now-compatible lookup\n",
        "        chunk['snaen'] = chunk['sno'].map(station_lookup)\n",
        "\n",
        "        # Fill any remaining missing values for stations that are truly decommissioned\n",
        "        chunk['snaen'].fillna('Decommissioned Station', inplace=True)\n",
        "\n",
        "        # Save the processed chunk\n",
        "        if chunk_num == 1:\n",
        "            chunk.to_csv(output_path, mode='w', index=False, header=True, encoding='utf-8-sig')\n",
        "        else:\n",
        "            chunk.to_csv(output_path, mode='a', index=False, header=False, encoding='utf-8-sig')\n",
        "\n",
        "        chunk_num += 1\n",
        "\n",
        "    print(\"\\n✓ All chunks processed and saved.\")\n",
        "    print(f\"\\n✅ Success! The updated data has been saved to:\\n{output_path}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"❌ Error: The file '{input_path}' was not found.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during chunk processing: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jWx85iuMkXFl",
        "outputId": "767f6e9f-e7cf-4bf9-e0fd-ffff8b443e58"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connecting to Google Drive...\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "\n",
            "Fetching master station list from the YouBike API...\n",
            "✓ Created a lookup map for 1647 unique stations.\n",
            "\n",
            "Processing your large dataset from: /content/drive/MyDrive/Youbike_Master_Project/YouBike_Demand_Forecast/data_clean/consolidated_youbike_data_processed.csv\n",
            "  -> Processing chunk 1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2816492037.py:59: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  chunk['snaen'].fillna('Decommissioned Station', inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Processing chunk 2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2816492037.py:59: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  chunk['snaen'].fillna('Decommissioned Station', inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Processing chunk 3...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2816492037.py:59: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  chunk['snaen'].fillna('Decommissioned Station', inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Processing chunk 4...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2816492037.py:59: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  chunk['snaen'].fillna('Decommissioned Station', inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Processing chunk 5...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2816492037.py:59: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  chunk['snaen'].fillna('Decommissioned Station', inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Processing chunk 6...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2816492037.py:59: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  chunk['snaen'].fillna('Decommissioned Station', inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Processing chunk 7...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2816492037.py:59: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  chunk['snaen'].fillna('Decommissioned Station', inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Processing chunk 8...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2816492037.py:59: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  chunk['snaen'].fillna('Decommissioned Station', inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Processing chunk 9...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2816492037.py:59: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  chunk['snaen'].fillna('Decommissioned Station', inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Processing chunk 10...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2816492037.py:59: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  chunk['snaen'].fillna('Decommissioned Station', inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Processing chunk 11...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2816492037.py:59: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  chunk['snaen'].fillna('Decommissioned Station', inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Processing chunk 12...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2816492037.py:59: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  chunk['snaen'].fillna('Decommissioned Station', inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Processing chunk 13...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2816492037.py:59: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  chunk['snaen'].fillna('Decommissioned Station', inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Processing chunk 14...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2816492037.py:59: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  chunk['snaen'].fillna('Decommissioned Station', inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Processing chunk 15...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2816492037.py:59: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  chunk['snaen'].fillna('Decommissioned Station', inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Processing chunk 16...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2816492037.py:59: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  chunk['snaen'].fillna('Decommissioned Station', inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Processing chunk 17...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2816492037.py:59: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  chunk['snaen'].fillna('Decommissioned Station', inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Processing chunk 18...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2816492037.py:59: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  chunk['snaen'].fillna('Decommissioned Station', inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Processing chunk 19...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2816492037.py:59: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  chunk['snaen'].fillna('Decommissioned Station', inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Processing chunk 20...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2816492037.py:59: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  chunk['snaen'].fillna('Decommissioned Station', inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Processing chunk 21...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2816492037.py:59: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  chunk['snaen'].fillna('Decommissioned Station', inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Processing chunk 22...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2816492037.py:59: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  chunk['snaen'].fillna('Decommissioned Station', inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Processing chunk 23...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2816492037.py:59: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  chunk['snaen'].fillna('Decommissioned Station', inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Processing chunk 24...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2816492037.py:59: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  chunk['snaen'].fillna('Decommissioned Station', inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Processing chunk 25...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2816492037.py:59: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  chunk['snaen'].fillna('Decommissioned Station', inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Processing chunk 26...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2816492037.py:59: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  chunk['snaen'].fillna('Decommissioned Station', inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Processing chunk 27...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2816492037.py:59: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  chunk['snaen'].fillna('Decommissioned Station', inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Processing chunk 28...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2816492037.py:59: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  chunk['snaen'].fillna('Decommissioned Station', inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Processing chunk 29...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2816492037.py:59: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  chunk['snaen'].fillna('Decommissioned Station', inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Processing chunk 30...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2816492037.py:59: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  chunk['snaen'].fillna('Decommissioned Station', inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Processing chunk 31...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2816492037.py:59: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  chunk['snaen'].fillna('Decommissioned Station', inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Processing chunk 32...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2816492037.py:59: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  chunk['snaen'].fillna('Decommissioned Station', inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Processing chunk 33...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2816492037.py:59: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  chunk['snaen'].fillna('Decommissioned Station', inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Processing chunk 34...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2816492037.py:59: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  chunk['snaen'].fillna('Decommissioned Station', inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Processing chunk 35...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2816492037.py:59: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  chunk['snaen'].fillna('Decommissioned Station', inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Processing chunk 36...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2816492037.py:59: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  chunk['snaen'].fillna('Decommissioned Station', inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Processing chunk 37...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2816492037.py:59: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  chunk['snaen'].fillna('Decommissioned Station', inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Processing chunk 38...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2816492037.py:59: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  chunk['snaen'].fillna('Decommissioned Station', inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Processing chunk 39...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2816492037.py:59: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  chunk['snaen'].fillna('Decommissioned Station', inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Processing chunk 40...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2816492037.py:59: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  chunk['snaen'].fillna('Decommissioned Station', inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Processing chunk 41...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2816492037.py:59: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  chunk['snaen'].fillna('Decommissioned Station', inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Processing chunk 42...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2816492037.py:59: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  chunk['snaen'].fillna('Decommissioned Station', inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Processing chunk 43...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2816492037.py:59: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  chunk['snaen'].fillna('Decommissioned Station', inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Processing chunk 44...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2816492037.py:59: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  chunk['snaen'].fillna('Decommissioned Station', inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Processing chunk 45...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2816492037.py:59: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  chunk['snaen'].fillna('Decommissioned Station', inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Processing chunk 46...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2816492037.py:59: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  chunk['snaen'].fillna('Decommissioned Station', inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Processing chunk 47...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2816492037.py:59: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  chunk['snaen'].fillna('Decommissioned Station', inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Processing chunk 48...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2816492037.py:59: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  chunk['snaen'].fillna('Decommissioned Station', inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Processing chunk 49...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2816492037.py:59: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  chunk['snaen'].fillna('Decommissioned Station', inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Processing chunk 50...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2816492037.py:59: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  chunk['snaen'].fillna('Decommissioned Station', inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Processing chunk 51...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2816492037.py:59: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  chunk['snaen'].fillna('Decommissioned Station', inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Processing chunk 52...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2816492037.py:59: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  chunk['snaen'].fillna('Decommissioned Station', inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Processing chunk 53...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2816492037.py:59: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  chunk['snaen'].fillna('Decommissioned Station', inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Processing chunk 54...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2816492037.py:59: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  chunk['snaen'].fillna('Decommissioned Station', inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Processing chunk 55...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2816492037.py:59: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  chunk['snaen'].fillna('Decommissioned Station', inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Processing chunk 56...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2816492037.py:59: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  chunk['snaen'].fillna('Decommissioned Station', inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Processing chunk 57...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2816492037.py:59: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  chunk['snaen'].fillna('Decommissioned Station', inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Processing chunk 58...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2816492037.py:59: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  chunk['snaen'].fillna('Decommissioned Station', inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Processing chunk 59...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2816492037.py:59: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  chunk['snaen'].fillna('Decommissioned Station', inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Processing chunk 60...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2816492037.py:59: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  chunk['snaen'].fillna('Decommissioned Station', inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Processing chunk 61...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2816492037.py:59: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  chunk['snaen'].fillna('Decommissioned Station', inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Processing chunk 62...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2816492037.py:59: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  chunk['snaen'].fillna('Decommissioned Station', inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Processing chunk 63...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2816492037.py:59: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  chunk['snaen'].fillna('Decommissioned Station', inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Processing chunk 64...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2816492037.py:59: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  chunk['snaen'].fillna('Decommissioned Station', inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Processing chunk 65...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2816492037.py:59: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  chunk['snaen'].fillna('Decommissioned Station', inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Processing chunk 66...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2816492037.py:59: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  chunk['snaen'].fillna('Decommissioned Station', inplace=True)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2816492037.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mchunk_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlow_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchunk_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"  -> Processing chunk {chunk_num}...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1841\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1842\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1843\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1844\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1845\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mget_chunk\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m   1983\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1984\u001b[0m             \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnrows\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_currow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1985\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1987\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mSelf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1921\u001b[0m                     \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1922\u001b[0m                     \u001b[0mcol_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m                 \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1924\u001b[0m                     \u001b[0mnrows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1925\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# --- 1. Setup Paths and Mount Drive ---\n",
        "print(\"Connecting to Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "CLEAN_DATA_DIR = '/content/drive/MyDrive/Youbike_Master_Project/YouBike_Demand_Forecast/data_clean/'\n",
        "input_filename = 'consolidated_youbike_data_processed.csv'\n",
        "output_filename = 'consolidated_data_with_snaen_corrected.csv'\n",
        "\n",
        "input_path = os.path.join(CLEAN_DATA_DIR, input_filename)\n",
        "output_path = os.path.join(CLEAN_DATA_DIR, output_filename)\n",
        "\n",
        "\n",
        "# --- 2. Fetch Master Station List and Standardize Data Type ---\n",
        "url = \"https://tcgbusfs.blob.core.windows.net/dotapp/youbike/v2/youbike_immediate.json\"\n",
        "print(\"\\nFetching master station list from the YouBike API...\")\n",
        "\n",
        "try:\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()\n",
        "    api_data = response.json()\n",
        "\n",
        "    station_map_df = pd.DataFrame(api_data)\n",
        "\n",
        "    # FIX 1: Convert API 'sno' to string\n",
        "    station_map_df['sno'] = station_map_df['sno'].astype(str)\n",
        "\n",
        "    station_map_df['snaen'] = station_map_df['snaen'].str.removeprefix('YouBike2.0_')\n",
        "\n",
        "    station_lookup = station_map_df.set_index('sno')['snaen'].to_dict()\n",
        "    print(f\"✓ Created a lookup map for {len(station_lookup)} unique stations.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error fetching or processing API data: {e}\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "# --- 3. Process the ENTIRE Large CSV in Chunks ---\n",
        "print(f\"\\nProcessing your large dataset...\")\n",
        "try:\n",
        "    chunk_num = 1\n",
        "    chunk_iterator = pd.read_csv(input_path, chunksize=500000, low_memory=False)\n",
        "\n",
        "    for chunk in chunk_iterator:\n",
        "        print(f\"  -> Processing chunk {chunk_num}...\")\n",
        "\n",
        "        # FIX 2: Convert 'sno' in your data chunk to string before mapping\n",
        "        chunk['sno'] = chunk['sno'].astype(str)\n",
        "\n",
        "        # Add the 'snaen' column using the now-compatible lookup\n",
        "        chunk['snaen'] = chunk['sno'].map(station_lookup)\n",
        "\n",
        "        # Fill any remaining missing values for stations that are truly decommissioned\n",
        "        chunk['snaen'] = chunk['snaen'].fillna('Decommissioned Station')\n",
        "\n",
        "        # Save the processed chunk\n",
        "        if chunk_num == 1:\n",
        "            chunk.to_csv(output_path, mode='w', index=False, header=True, encoding='utf-8-sig')\n",
        "        else:\n",
        "            chunk.to_csv(output_path, mode='a', index=False, header=False, encoding='utf-8-sig')\n",
        "\n",
        "        chunk_num += 1\n",
        "\n",
        "    print(\"\\n✓ All chunks processed and saved.\")\n",
        "    print(f\"\\n✅ Success! The updated data has been saved to:\\n{output_path}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"❌ Error: The file '{input_path}' was not found.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during chunk processing: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "490qFMKGvtUm",
        "outputId": "b5e6e069-3f72-47ae-d715-14c8415cc791"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connecting to Google Drive...\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "\n",
            "Fetching master station list from the YouBike API...\n",
            "✓ Created a lookup map for 1647 unique stations.\n",
            "\n",
            "Processing your large dataset...\n",
            "  -> Processing chunk 1...\n",
            "  -> Processing chunk 2...\n",
            "  -> Processing chunk 3...\n",
            "  -> Processing chunk 4...\n",
            "  -> Processing chunk 5...\n",
            "  -> Processing chunk 6...\n",
            "  -> Processing chunk 7...\n",
            "  -> Processing chunk 8...\n",
            "  -> Processing chunk 9...\n",
            "  -> Processing chunk 10...\n",
            "  -> Processing chunk 11...\n",
            "  -> Processing chunk 12...\n",
            "  -> Processing chunk 13...\n",
            "  -> Processing chunk 14...\n",
            "  -> Processing chunk 15...\n",
            "  -> Processing chunk 16...\n",
            "  -> Processing chunk 17...\n",
            "  -> Processing chunk 18...\n",
            "  -> Processing chunk 19...\n",
            "  -> Processing chunk 20...\n",
            "  -> Processing chunk 21...\n",
            "  -> Processing chunk 22...\n",
            "  -> Processing chunk 23...\n",
            "  -> Processing chunk 24...\n",
            "  -> Processing chunk 25...\n",
            "  -> Processing chunk 26...\n",
            "  -> Processing chunk 27...\n",
            "  -> Processing chunk 28...\n",
            "  -> Processing chunk 29...\n",
            "  -> Processing chunk 30...\n",
            "  -> Processing chunk 31...\n",
            "  -> Processing chunk 32...\n",
            "  -> Processing chunk 33...\n",
            "  -> Processing chunk 34...\n",
            "  -> Processing chunk 35...\n",
            "  -> Processing chunk 36...\n",
            "  -> Processing chunk 37...\n",
            "  -> Processing chunk 38...\n",
            "  -> Processing chunk 39...\n",
            "  -> Processing chunk 40...\n",
            "  -> Processing chunk 41...\n",
            "  -> Processing chunk 42...\n",
            "  -> Processing chunk 43...\n",
            "  -> Processing chunk 44...\n",
            "  -> Processing chunk 45...\n",
            "  -> Processing chunk 46...\n",
            "  -> Processing chunk 47...\n",
            "  -> Processing chunk 48...\n",
            "  -> Processing chunk 49...\n",
            "  -> Processing chunk 50...\n",
            "  -> Processing chunk 51...\n",
            "  -> Processing chunk 52...\n",
            "  -> Processing chunk 53...\n",
            "  -> Processing chunk 54...\n",
            "  -> Processing chunk 55...\n",
            "  -> Processing chunk 56...\n",
            "  -> Processing chunk 57...\n",
            "  -> Processing chunk 58...\n",
            "  -> Processing chunk 59...\n",
            "  -> Processing chunk 60...\n",
            "  -> Processing chunk 61...\n",
            "  -> Processing chunk 62...\n",
            "  -> Processing chunk 63...\n",
            "  -> Processing chunk 64...\n",
            "  -> Processing chunk 65...\n",
            "  -> Processing chunk 66...\n",
            "  -> Processing chunk 67...\n",
            "  -> Processing chunk 68...\n",
            "  -> Processing chunk 69...\n",
            "  -> Processing chunk 70...\n",
            "  -> Processing chunk 71...\n",
            "  -> Processing chunk 72...\n",
            "  -> Processing chunk 73...\n",
            "  -> Processing chunk 74...\n",
            "  -> Processing chunk 75...\n",
            "  -> Processing chunk 76...\n",
            "  -> Processing chunk 77...\n",
            "  -> Processing chunk 78...\n",
            "  -> Processing chunk 79...\n",
            "  -> Processing chunk 80...\n",
            "  -> Processing chunk 81...\n",
            "  -> Processing chunk 82...\n",
            "  -> Processing chunk 83...\n",
            "  -> Processing chunk 84...\n",
            "  -> Processing chunk 85...\n",
            "  -> Processing chunk 86...\n",
            "  -> Processing chunk 87...\n",
            "  -> Processing chunk 88...\n",
            "  -> Processing chunk 89...\n",
            "  -> Processing chunk 90...\n",
            "  -> Processing chunk 91...\n",
            "  -> Processing chunk 92...\n",
            "  -> Processing chunk 93...\n",
            "  -> Processing chunk 94...\n",
            "  -> Processing chunk 95...\n",
            "  -> Processing chunk 96...\n",
            "  -> Processing chunk 97...\n",
            "  -> Processing chunk 98...\n",
            "  -> Processing chunk 99...\n",
            "  -> Processing chunk 100...\n",
            "  -> Processing chunk 101...\n",
            "  -> Processing chunk 102...\n",
            "  -> Processing chunk 103...\n",
            "  -> Processing chunk 104...\n",
            "  -> Processing chunk 105...\n",
            "  -> Processing chunk 106...\n",
            "  -> Processing chunk 107...\n",
            "  -> Processing chunk 108...\n",
            "  -> Processing chunk 109...\n",
            "  -> Processing chunk 110...\n",
            "  -> Processing chunk 111...\n",
            "  -> Processing chunk 112...\n",
            "  -> Processing chunk 113...\n",
            "  -> Processing chunk 114...\n",
            "  -> Processing chunk 115...\n",
            "  -> Processing chunk 116...\n",
            "  -> Processing chunk 117...\n",
            "  -> Processing chunk 118...\n",
            "  -> Processing chunk 119...\n",
            "  -> Processing chunk 120...\n",
            "  -> Processing chunk 121...\n",
            "  -> Processing chunk 122...\n",
            "  -> Processing chunk 123...\n",
            "  -> Processing chunk 124...\n",
            "  -> Processing chunk 125...\n",
            "  -> Processing chunk 126...\n",
            "  -> Processing chunk 127...\n",
            "  -> Processing chunk 128...\n",
            "  -> Processing chunk 129...\n",
            "  -> Processing chunk 130...\n",
            "  -> Processing chunk 131...\n",
            "  -> Processing chunk 132...\n",
            "  -> Processing chunk 133...\n",
            "  -> Processing chunk 134...\n",
            "  -> Processing chunk 135...\n",
            "  -> Processing chunk 136...\n",
            "  -> Processing chunk 137...\n",
            "  -> Processing chunk 138...\n",
            "  -> Processing chunk 139...\n",
            "  -> Processing chunk 140...\n",
            "  -> Processing chunk 141...\n",
            "  -> Processing chunk 142...\n",
            "  -> Processing chunk 143...\n",
            "  -> Processing chunk 144...\n",
            "  -> Processing chunk 145...\n",
            "  -> Processing chunk 146...\n",
            "  -> Processing chunk 147...\n",
            "  -> Processing chunk 148...\n",
            "  -> Processing chunk 149...\n",
            "  -> Processing chunk 150...\n",
            "  -> Processing chunk 151...\n",
            "  -> Processing chunk 152...\n",
            "  -> Processing chunk 153...\n",
            "  -> Processing chunk 154...\n",
            "  -> Processing chunk 155...\n",
            "  -> Processing chunk 156...\n",
            "  -> Processing chunk 157...\n",
            "  -> Processing chunk 158...\n",
            "  -> Processing chunk 159...\n",
            "  -> Processing chunk 160...\n",
            "  -> Processing chunk 161...\n",
            "  -> Processing chunk 162...\n",
            "  -> Processing chunk 163...\n",
            "  -> Processing chunk 164...\n",
            "  -> Processing chunk 165...\n",
            "  -> Processing chunk 166...\n",
            "  -> Processing chunk 167...\n",
            "  -> Processing chunk 168...\n",
            "  -> Processing chunk 169...\n",
            "  -> Processing chunk 170...\n",
            "  -> Processing chunk 171...\n",
            "  -> Processing chunk 172...\n",
            "  -> Processing chunk 173...\n",
            "  -> Processing chunk 174...\n",
            "  -> Processing chunk 175...\n",
            "  -> Processing chunk 176...\n",
            "  -> Processing chunk 177...\n",
            "  -> Processing chunk 178...\n",
            "\n",
            "✓ All chunks processed and saved.\n",
            "\n",
            "✅ Success! The updated data has been saved to:\n",
            "/content/drive/MyDrive/Youbike_Master_Project/YouBike_Demand_Forecast/data_clean/consolidated_data_with_snaen_corrected.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# --- 1. Setup Paths and Mount Drive ---\n",
        "print(\"Connecting to Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "CLEAN_DATA_DIR = '/content/drive/MyDrive/Youbike_Master_Project/YouBike_Demand_Forecast/data_clean/'\n",
        "# The file you just created\n",
        "file_to_check = 'consolidated_data_with_snaen_corrected.csv'\n",
        "file_path = os.path.join(CLEAN_DATA_DIR, file_to_check)\n",
        "\n",
        "# --- 2. Load a Sample of the New File ---\n",
        "print(f\"\\nLoading a sample from your new file:\\n{file_path}\")\n",
        "try:\n",
        "    # Read just the first 10,000 rows to check\n",
        "    sample_df = pd.read_csv(file_path, nrows=10000)\n",
        "    print(\"✓ Sample loaded successfully.\")\n",
        "\n",
        "    # --- 3. Run Verification Checks ---\n",
        "    print(\"\\n--- Verification Report ---\")\n",
        "\n",
        "    # Check 1: Does the 'snaen' column exist?\n",
        "    if 'snaen' in sample_df.columns:\n",
        "        print(\"✅ Success: The 'snaen' column exists in the new file.\")\n",
        "    else:\n",
        "        print(\"❌ Failed: The 'snaen' column was NOT found.\")\n",
        "        exit() # Stop if the column isn't even there\n",
        "\n",
        "    # Check 2: How many stations were successfully matched in this sample?\n",
        "    total_rows = len(sample_df)\n",
        "    decommissioned_count = (sample_df['snaen'] == 'Decommissioned Station').sum()\n",
        "    successful_matches = total_rows - decommissioned_count\n",
        "\n",
        "    print(f\"✅ Success: Found {successful_matches} matched station names in the first {total_rows} rows.\")\n",
        "    if decommissioned_count > 0:\n",
        "        print(f\"   - Found {decommissioned_count} rows corresponding to decommissioned stations.\")\n",
        "\n",
        "    # Check 3: Show a visual preview\n",
        "    print(\"\\n--- Data Preview ---\")\n",
        "    print(\"Here are the first 10 rows of your new data:\")\n",
        "    print(sample_df[['sno', 'sna', 'snaen']].head(10))\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"❌ Error: The file '{file_path}' was not found. Please make sure the previous script ran successfully and the filename is correct.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mgdCah4w5vuM",
        "outputId": "735216cb-b277-4e2b-dbc5-c6270f1bad4c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connecting to Google Drive...\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "\n",
            "Loading a sample from your new file:\n",
            "/content/drive/MyDrive/Youbike_Master_Project/YouBike_Demand_Forecast/data_clean/consolidated_data_with_snaen_corrected.csv\n",
            "✓ Sample loaded successfully.\n",
            "\n",
            "--- Verification Report ---\n",
            "✅ Success: The 'snaen' column exists in the new file.\n",
            "✅ Success: Found 10000 matched station names in the first 10000 rows.\n",
            "\n",
            "--- Data Preview ---\n",
            "Here are the first 10 rows of your new data:\n",
            "         sno                 sna                      snaen\n",
            "0  500101001  YouBike2.0_捷運科技大樓站  MRT Technology Bldg. Sta.\n",
            "1  500101001  YouBike2.0_捷運科技大樓站  MRT Technology Bldg. Sta.\n",
            "2  500101001  YouBike2.0_捷運科技大樓站  MRT Technology Bldg. Sta.\n",
            "3  500101001  YouBike2.0_捷運科技大樓站  MRT Technology Bldg. Sta.\n",
            "4  500101001  YouBike2.0_捷運科技大樓站  MRT Technology Bldg. Sta.\n",
            "5  500101001  YouBike2.0_捷運科技大樓站  MRT Technology Bldg. Sta.\n",
            "6  500101001  YouBike2.0_捷運科技大樓站  MRT Technology Bldg. Sta.\n",
            "7  500101001  YouBike2.0_捷運科技大樓站  MRT Technology Bldg. Sta.\n",
            "8  500101001  YouBike2.0_捷運科技大樓站  MRT Technology Bldg. Sta.\n",
            "9  500101001  YouBike2.0_捷運科技大樓站  MRT Technology Bldg. Sta.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# --- 1. Setup Paths and Mount Drive ---\n",
        "print(\"Connecting to Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "CLEAN_DATA_DIR = '/content/drive/MyDrive/Youbike_Master_Project/YouBike_Demand_Forecast/data_clean/'\n",
        "input_filename = 'consolidated_youbike_data_processed.csv'\n",
        "output_filename = 'consolidated_data_with_snaen_corrected.csv'\n",
        "\n",
        "input_path = os.path.join(CLEAN_DATA_DIR, input_filename)\n",
        "output_path = os.path.join(CLEAN_DATA_DIR, output_filename)\n",
        "\n",
        "\n",
        "# --- 2. Fetch Master Station List and Standardize Data Type ---\n",
        "url = \"https://tcgbusfs.blob.core.windows.net/dotapp/youbike/v2/youbike_immediate.json\"\n",
        "print(\"\\nFetching master station list from the YouBike API...\")\n",
        "\n",
        "try:\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()\n",
        "    api_data = response.json()\n",
        "\n",
        "    station_map_df = pd.DataFrame(api_data)\n",
        "    station_map_df['sno'] = station_map_df['sno'].astype(str)\n",
        "    station_map_df['snaen'] = station_map_df['snaen'].str.removeprefix('YouBike2.0_')\n",
        "\n",
        "    station_lookup = station_map_df.set_index('sno')['snaen'].to_dict()\n",
        "    print(f\"✓ Created a lookup map for {len(station_lookup)} unique stations.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error fetching or processing API data: {e}\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "# --- 3. Process the Large CSV in Larger Chunks ---\n",
        "print(f\"\\nProcessing your large dataset with an increased chunk size...\")\n",
        "try:\n",
        "    chunk_num = 1\n",
        "    # **OPTIMIZATION: Increased chunksize from 500,000 to 2,000,000**\n",
        "    chunk_iterator = pd.read_csv(input_path, chunksize=2000000, low_memory=False)\n",
        "\n",
        "    for chunk in chunk_iterator:\n",
        "        print(f\"  -> Processing chunk {chunk_num}...\")\n",
        "\n",
        "        chunk['sno'] = chunk['sno'].astype(str)\n",
        "        chunk['snaen'] = chunk['sno'].map(station_lookup)\n",
        "        chunk['snaen'] = chunk['snaen'].fillna('Decommissioned Station')\n",
        "\n",
        "        if chunk_num == 1:\n",
        "            chunk.to_csv(output_path, mode='w', index=False, header=True, encoding='utf-8-sig')\n",
        "        else:\n",
        "            chunk.to_csv(output_path, mode='a', index=False, header=False, encoding='utf-8-sig')\n",
        "\n",
        "        chunk_num += 1\n",
        "\n",
        "    print(\"\\n✓ All chunks processed and saved.\")\n",
        "    print(f\"\\n✅ Success! The updated data has been saved to:\\n{output_path}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"❌ Error: The file '{input_path}' was not found.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during chunk processing: {e}\")"
      ],
      "metadata": {
        "id": "Tu_aw9Fd8aV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# --- 1. Setup Paths and Mount Drive ---\n",
        "print(\"Connecting to Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "CLEAN_DATA_DIR = '/content/drive/MyDrive/Youbike_Master_Project/YouBike_Demand_Forecast/data_clean/'\n",
        "file_to_check = 'consolidated_data_with_snaen_corrected.csv'\n",
        "file_path = os.path.join(CLEAN_DATA_DIR, file_to_check)\n",
        "\n",
        "# --- 2. Process the ENTIRE File in Chunks for Verification ---\n",
        "print(f\"\\nStarting full verification of file:\\n{file_path}\")\n",
        "try:\n",
        "    # Initialize counters for the final report\n",
        "    total_rows_processed = 0\n",
        "    total_decommissioned = 0\n",
        "    chunk_num = 1\n",
        "\n",
        "    # Create an iterator to read the CSV in chunks of 2,000,000 rows\n",
        "    chunk_iterator = pd.read_csv(file_path, chunksize=2000000, low_memory=False)\n",
        "\n",
        "    for chunk in chunk_iterator:\n",
        "        print(f\"  -> Verifying chunk {chunk_num}...\")\n",
        "\n",
        "        # Check if 'snaen' column exists in this chunk\n",
        "        if 'snaen' not in chunk.columns:\n",
        "            print(\"❌ Critical Error: 'snaen' column is missing in this chunk. Stopping.\")\n",
        "            break\n",
        "\n",
        "        # Update the total counts\n",
        "        total_rows_processed += len(chunk)\n",
        "        total_decommissioned += (chunk['snaen'] == 'Decommissioned Station').sum()\n",
        "\n",
        "        chunk_num += 1\n",
        "\n",
        "    # Calculate the final numbers\n",
        "    total_successful = total_rows_processed - total_decommissioned\n",
        "\n",
        "    # --- 3. Display the Final, Aggregated Report ---\n",
        "    print(\"\\n--- Full File Verification Report ---\")\n",
        "    print(f\"✅ Total rows processed: {total_rows_processed:,}\")\n",
        "    print(f\"✅ Total successful matches found: {total_successful:,}\")\n",
        "    print(f\"ℹ️  Total rows for decommissioned stations: {total_decommissioned:,}\")\n",
        "\n",
        "    if total_rows_processed > 0:\n",
        "        success_rate = (total_successful / total_rows_processed) * 100\n",
        "        print(f\"   -> Match Rate: {success_rate:.4f}%\")\n",
        "\n",
        "    print(\"\\n✅ Verification of the entire file is complete.\")\n",
        "\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"❌ Error: The file '{file_path}' was not found.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0hawqx_7-efS",
        "outputId": "54f436cc-4b0e-45be-ac74-bce63bbda954"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connecting to Google Drive...\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "\n",
            "Starting full verification of file:\n",
            "/content/drive/MyDrive/Youbike_Master_Project/YouBike_Demand_Forecast/data_clean/consolidated_data_with_snaen_corrected.csv\n",
            "  -> Verifying chunk 1...\n",
            "  -> Verifying chunk 2...\n",
            "  -> Verifying chunk 3...\n",
            "  -> Verifying chunk 4...\n",
            "  -> Verifying chunk 5...\n",
            "  -> Verifying chunk 6...\n",
            "  -> Verifying chunk 7...\n",
            "  -> Verifying chunk 8...\n",
            "  -> Verifying chunk 9...\n",
            "  -> Verifying chunk 10...\n",
            "  -> Verifying chunk 11...\n",
            "  -> Verifying chunk 12...\n",
            "  -> Verifying chunk 13...\n",
            "  -> Verifying chunk 14...\n",
            "  -> Verifying chunk 15...\n",
            "  -> Verifying chunk 16...\n",
            "  -> Verifying chunk 17...\n",
            "  -> Verifying chunk 18...\n",
            "  -> Verifying chunk 19...\n",
            "  -> Verifying chunk 20...\n",
            "  -> Verifying chunk 21...\n",
            "  -> Verifying chunk 22...\n",
            "  -> Verifying chunk 23...\n",
            "  -> Verifying chunk 24...\n",
            "  -> Verifying chunk 25...\n",
            "  -> Verifying chunk 26...\n",
            "  -> Verifying chunk 27...\n",
            "  -> Verifying chunk 28...\n",
            "  -> Verifying chunk 29...\n",
            "  -> Verifying chunk 30...\n",
            "  -> Verifying chunk 31...\n",
            "  -> Verifying chunk 32...\n",
            "  -> Verifying chunk 33...\n",
            "  -> Verifying chunk 34...\n",
            "  -> Verifying chunk 35...\n",
            "  -> Verifying chunk 36...\n",
            "  -> Verifying chunk 37...\n",
            "  -> Verifying chunk 38...\n",
            "  -> Verifying chunk 39...\n",
            "  -> Verifying chunk 40...\n",
            "  -> Verifying chunk 41...\n",
            "  -> Verifying chunk 42...\n",
            "  -> Verifying chunk 43...\n",
            "  -> Verifying chunk 44...\n",
            "  -> Verifying chunk 45...\n",
            "\n",
            "--- Full File Verification Report ---\n",
            "✅ Total rows processed: 88,611,411\n",
            "✅ Total successful matches found: 88,491,881\n",
            "ℹ️  Total rows for decommissioned stations: 119,530\n",
            "   -> Match Rate: 99.8651%\n",
            "\n",
            "✅ Verification of the entire file is complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# --- 1. Setup Paths and Mount Drive ---\n",
        "print(\"Connecting to Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "CLEAN_DATA_DIR = '/content/drive/MyDrive/Youbike_Master_Project/YouBike_Demand_Forecast/data_clean/'\n",
        "file_to_analyze = 'consolidated_data_with_snaen_corrected.csv'\n",
        "file_path = os.path.join(CLEAN_DATA_DIR, file_to_analyze)\n",
        "\n",
        "# --- 2. Process the Entire File in Chunks to Find Decommissioned Stations ---\n",
        "print(f\"\\nScanning the entire file for decommissioned stations...\")\n",
        "try:\n",
        "    # Use a dictionary to store the unique sno -> sna mapping for decommissioned stations\n",
        "    decommissioned_stations = {}\n",
        "    chunk_num = 1\n",
        "\n",
        "    # Create an iterator to read the CSV in chunks\n",
        "    chunk_iterator = pd.read_csv(file_path, chunksize=6000000, low_memory=False)\n",
        "\n",
        "    for chunk in chunk_iterator:\n",
        "        print(f\"  -> Scanning chunk {chunk_num}...\")\n",
        "\n",
        "        # Filter the chunk to find rows where 'snaen' is 'Decommissioned Station'\n",
        "        decommissioned_chunk = chunk[chunk['snaen'] == 'Decommissioned Station']\n",
        "\n",
        "        # If any are found, add their unique 'sno' and 'sna' to our dictionary\n",
        "        if not decommissioned_chunk.empty:\n",
        "            # Drop duplicates to only get unique sno/sna pairs from this chunk\n",
        "            unique_in_chunk = decommissioned_chunk[['sno', 'sna']].drop_duplicates()\n",
        "            # Update the master dictionary\n",
        "            for index, row in unique_in_chunk.iterrows():\n",
        "                if row['sno'] not in decommissioned_stations:\n",
        "                    decommissioned_stations[row['sno']] = row['sna']\n",
        "\n",
        "        chunk_num += 1\n",
        "\n",
        "    # --- 3. Display the Final Report ---\n",
        "    print(\"\\n--- Decommissioned Stations Report ---\")\n",
        "    if decommissioned_stations:\n",
        "        print(f\"✅ Found {len(decommissioned_stations)} unique decommissioned stations in the entire file.\")\n",
        "\n",
        "        # Convert the dictionary to a DataFrame for nice printing\n",
        "        decommissioned_df = pd.DataFrame(list(decommissioned_stations.items()), columns=['SNO', 'Original SNA (Chinese Name)'])\n",
        "\n",
        "        print(\"\\nHere is the complete list:\")\n",
        "        print(decommissioned_df.to_string())\n",
        "    else:\n",
        "        print(\"✅ No decommissioned stations found in the entire file.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"❌ Error: The file '{file_path}' was not found.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-CdJj-uLBaH7",
        "outputId": "ce04c000-8fc2-455a-e3c7-0804595c33d2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connecting to Google Drive...\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "\n",
            "Scanning the entire file for decommissioned stations...\n",
            "  -> Scanning chunk 1...\n",
            "  -> Scanning chunk 2...\n",
            "  -> Scanning chunk 3...\n",
            "  -> Scanning chunk 4...\n",
            "  -> Scanning chunk 5...\n",
            "  -> Scanning chunk 6...\n",
            "  -> Scanning chunk 7...\n",
            "  -> Scanning chunk 8...\n",
            "  -> Scanning chunk 9...\n",
            "  -> Scanning chunk 10...\n",
            "  -> Scanning chunk 11...\n",
            "  -> Scanning chunk 12...\n",
            "  -> Scanning chunk 13...\n",
            "  -> Scanning chunk 14...\n",
            "  -> Scanning chunk 15...\n",
            "\n",
            "--- Decommissioned Stations Report ---\n",
            "✅ Found 6 unique decommissioned stations in the entire file.\n",
            "\n",
            "Here is the complete list:\n",
            "         SNO Original SNA (Chinese Name)\n",
            "0  500101183      YouBike2.0_忠孝東路三段217巷口\n",
            "1  500105052      YouBike2.0_國立政治大學(萬壽路)\n",
            "2  500107057           YouBike2.0_中山公民會館\n",
            "3  500107074           YouBike2.0_中原民生路口\n",
            "4  500108130          YouBike2.0_行善路383巷\n",
            "5  500106141        YouBike2.0_愛國東路116巷口\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "import sys\n",
        "\n",
        "def check_youbike_stations():\n",
        "    \"\"\"\n",
        "    Downloads the YouBike station data and checks if a list of\n",
        "    specified station IDs are present in the dataset.\n",
        "    \"\"\"\n",
        "    # URL for the YouBike 2.0 immediate data API\n",
        "    url = \"https://tcgbusfs.blob.core.windows.net/dotapp/youbike/v2/youbike_immediate.json\"\n",
        "\n",
        "    # The list of station IDs provided by the user to check\n",
        "    # Note: These are converted to strings to match the data type in the JSON.\n",
        "    stations_to_check = [\n",
        "        \"500101183\",\n",
        "        \"500105052\",\n",
        "        \"500107057\",\n",
        "        \"500107074\",\n",
        "        \"500108130\",\n",
        "        \"500106141\"\n",
        "    ]\n",
        "\n",
        "    print(\"Fetching YouBike station data from the API...\")\n",
        "\n",
        "    try:\n",
        "        # Send a GET request to the URL\n",
        "        response = requests.get(url, timeout=10)\n",
        "\n",
        "        # Raise an HTTPError for bad responses (4xx or 5xx)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        # Parse the JSON content from the response\n",
        "        data = response.json()\n",
        "\n",
        "        # Determine if the data is a dictionary or a list\n",
        "        if isinstance(data, dict) and 'retVal' in data:\n",
        "            stations = data['retVal']\n",
        "        elif isinstance(data, list):\n",
        "            stations = data\n",
        "        else:\n",
        "            print(\"The API response is in an unexpected format.\", file=sys.stderr)\n",
        "            return\n",
        "\n",
        "        # A set is used for efficient lookup (O(1) average time complexity)\n",
        "        # We extract the 'sno' (station number) from each station entry\n",
        "        existing_station_ids = {station['sno'] for station in stations}\n",
        "\n",
        "        print(\"\\n--- Checking Stations ---\\n\")\n",
        "\n",
        "        # Check each station ID from the list\n",
        "        for station_id in stations_to_check:\n",
        "            if station_id in existing_station_ids:\n",
        "                print(f\"✅ Station ID '{station_id}' was found in the data.\")\n",
        "            else:\n",
        "                print(f\"❌ Station ID '{station_id}' was NOT found in the data.\")\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"An error occurred while fetching the data: {e}\", file=sys.stderr)\n",
        "        print(\"Please check your internet connection or the API URL.\", file=sys.stderr)\n",
        "\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"An error occurred while parsing the JSON data: {e}\", file=sys.stderr)\n",
        "        print(\"The data received from the API may be in an incorrect format.\", file=sys.stderr)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\", file=sys.stderr)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    check_youbike_stations()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scm6bdexF5C1",
        "outputId": "e5260d7f-b135-491a-ad6f-f7d7fe8eeb21"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching YouBike station data from the API...\n",
            "\n",
            "--- Checking Stations ---\n",
            "\n",
            "❌ Station ID '500101183' was NOT found in the data.\n",
            "❌ Station ID '500105052' was NOT found in the data.\n",
            "❌ Station ID '500107057' was NOT found in the data.\n",
            "❌ Station ID '500107074' was NOT found in the data.\n",
            "❌ Station ID '500108130' was NOT found in the data.\n",
            "❌ Station ID '500106141' was NOT found in the data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import sys\n",
        "from google.colab import drive\n",
        "\n",
        "print(\"Connecting to Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the directory and file path as provided\n",
        "CLEAN_DATA_DIR = '/content/drive/MyDrive/Youbike_Master_Project/YouBike_Demand_Forecast/data_clean/'\n",
        "file_to_analyze = 'consolidated_data_with_snaen_corrected.csv'\n",
        "filepath = os.path.join(CLEAN_DATA_DIR, file_to_analyze)\n",
        "\n",
        "# The list of station IDs to be deleted from the data\n",
        "stations_to_delete = [\n",
        "    \"500101183\",\n",
        "    \"500105052\",\n",
        "    \"500107057\",\n",
        "    \"500107074\",\n",
        "    \"500108130\",\n",
        "    \"500106141\"\n",
        "]\n",
        "\n",
        "def remove_stations_from_csv_in_batches():\n",
        "    \"\"\"\n",
        "    Reads a large CSV file in batches, removes specified station data,\n",
        "    and saves the result to a new file to avoid overwriting the original.\n",
        "    \"\"\"\n",
        "    print(f\"Reading data from: {filepath}\")\n",
        "\n",
        "    try:\n",
        "        # Create a new output file path with a '_cleaned' suffix\n",
        "        filename, file_extension = os.path.splitext(file_to_analyze)\n",
        "        new_filepath = os.path.join(CLEAN_DATA_DIR, f\"{filename}_cleaned{file_extension}\")\n",
        "\n",
        "        # Check if the input file exists\n",
        "        if not os.path.exists(filepath):\n",
        "            print(f\"Error: The input file was not found at {filepath}. Please check the path.\", file=sys.stderr)\n",
        "            return\n",
        "\n",
        "        # Initialize counters\n",
        "        total_rows_read = 0\n",
        "        total_rows_deleted = 0\n",
        "        batch_size = 6000000\n",
        "\n",
        "        # Read the CSV in chunks\n",
        "        chunk_iterator = pd.read_csv(filepath, chunksize=batch_size, iterator=True)\n",
        "\n",
        "        # We need a flag to handle the header\n",
        "        is_first_chunk = True\n",
        "\n",
        "        for batch_number, chunk in enumerate(chunk_iterator, 1):\n",
        "            # Check if the 'sno' column exists in the first chunk\n",
        "            if is_first_chunk and 'sno' not in chunk.columns:\n",
        "                print(\"Error: The 'sno' column was not found in the CSV file.\", file=sys.stderr)\n",
        "                return\n",
        "\n",
        "            # Filter the chunk to remove the specified stations\n",
        "            cleaned_chunk = chunk[~chunk['sno'].isin(stations_to_delete)]\n",
        "\n",
        "            # Write the cleaned chunk to the new file\n",
        "            # If it's the first chunk, write with header. Otherwise, append without header.\n",
        "            cleaned_chunk.to_csv(\n",
        "                new_filepath,\n",
        "                index=False,\n",
        "                mode='w' if is_first_chunk else 'a',\n",
        "                header=is_first_chunk\n",
        "            )\n",
        "\n",
        "            total_rows_read += len(chunk)\n",
        "            total_rows_deleted += len(chunk) - len(cleaned_chunk)\n",
        "\n",
        "            print(f\"Processed batch {batch_number}. Rows read: {len(chunk)}. Rows deleted: {len(chunk) - len(cleaned_chunk)}.\")\n",
        "\n",
        "            is_first_chunk = False\n",
        "\n",
        "        print(\"\\n--- Operation Complete ---\")\n",
        "        print(f\"Total rows read: {total_rows_read}\")\n",
        "        print(f\"Total rows deleted: {total_rows_deleted}\")\n",
        "        print(f\"Final number of rows: {total_rows_read - total_rows_deleted}\")\n",
        "        print(f\"Cleaned data saved to: {new_filepath}\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: The file was not found at {filepath}. Please check the path.\", file=sys.stderr)\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\", file=sys.stderr)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    remove_stations_from_csv_in_batches()\n"
      ],
      "metadata": {
        "id": "fksQXQhrlp6I",
        "outputId": "6320844c-6569-4b5a-9ce4-b441eaee8759",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connecting to Google Drive...\n",
            "Mounted at /content/drive\n",
            "Reading data from: /content/drive/MyDrive/Youbike_Master_Project/YouBike_Demand_Forecast/data_clean/consolidated_data_with_snaen_corrected.csv\n",
            "Processed batch 1. Rows read: 6000000. Rows deleted: 0.\n",
            "Processed batch 2. Rows read: 6000000. Rows deleted: 0.\n",
            "Processed batch 3. Rows read: 6000000. Rows deleted: 0.\n",
            "Processed batch 4. Rows read: 6000000. Rows deleted: 0.\n",
            "Processed batch 5. Rows read: 6000000. Rows deleted: 0.\n",
            "Processed batch 6. Rows read: 6000000. Rows deleted: 0.\n",
            "Processed batch 7. Rows read: 6000000. Rows deleted: 0.\n",
            "Processed batch 8. Rows read: 6000000. Rows deleted: 0.\n",
            "Processed batch 9. Rows read: 6000000. Rows deleted: 0.\n",
            "Processed batch 10. Rows read: 6000000. Rows deleted: 0.\n",
            "Processed batch 11. Rows read: 6000000. Rows deleted: 0.\n",
            "Processed batch 12. Rows read: 6000000. Rows deleted: 0.\n",
            "Processed batch 13. Rows read: 6000000. Rows deleted: 0.\n",
            "Processed batch 14. Rows read: 6000000. Rows deleted: 0.\n",
            "Processed batch 15. Rows read: 4611411. Rows deleted: 0.\n",
            "\n",
            "--- Operation Complete ---\n",
            "Total rows read: 88611411\n",
            "Total rows deleted: 0\n",
            "Final number of rows: 88611411\n",
            "Cleaned data saved to: /content/drive/MyDrive/Youbike_Master_Project/YouBike_Demand_Forecast/data_clean/consolidated_data_with_snaen_corrected_cleaned.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Define the directory and file path as provided\n",
        "CLEAN_DATA_DIR = '/content/drive/MyDrive/Youbike_Master_Project/YouBike_Demand_Forecast/data_clean/'\n",
        "file_to_analyze = 'ubike_consolidated_data.csv'\n",
        "filepath = os.path.join(CLEAN_DATA_DIR, file_to_analyze)\n",
        "\n",
        "# The list of station IDs to be deleted from the data\n",
        "stations_to_delete = [\n",
        "    \"500101183\",\n",
        "    \"500105052\",\n",
        "    \"500107057\",\n",
        "    \"500107074\",\n",
        "    \"500108130\",\n",
        "    \"500106141\"\n",
        "]\n",
        "\n",
        "def remove_stations_from_csv_in_batches():\n",
        "    \"\"\"\n",
        "    Reads a large CSV file in batches, removes specified station data,\n",
        "    and saves the result to a new file to avoid overwriting the original.\n",
        "    \"\"\"\n",
        "    print(f\"Reading data from: {filepath}\")\n",
        "\n",
        "    try:\n",
        "        # Create a new output file path with a '_cleaned' suffix\n",
        "        filename, file_extension = os.path.splitext(file_to_analyze)\n",
        "        new_filepath = os.path.join(CLEAN_DATA_DIR, f\"{filename}_cleaned{file_extension}\")\n",
        "\n",
        "        # Check if the input file exists\n",
        "        if not os.path.exists(filepath):\n",
        "            print(f\"Error: The input file was not found at {filepath}. Please check the path.\", file=sys.stderr)\n",
        "            return\n",
        "\n",
        "        # Initialize counters\n",
        "        total_rows_read = 0\n",
        "        total_rows_deleted = 0\n",
        "        batch_size = 12000000\n",
        "\n",
        "        # Read the CSV in chunks\n",
        "        chunk_iterator = pd.read_csv(filepath, chunksize=batch_size, iterator=True)\n",
        "\n",
        "        # We need a flag to handle the header\n",
        "        is_first_chunk = True\n",
        "\n",
        "        for batch_number, chunk in enumerate(chunk_iterator, 1):\n",
        "            # Check if the 'sno' column exists in the first chunk\n",
        "            if is_first_chunk and 'sno' not in chunk.columns:\n",
        "                print(\"Error: The 'sno' column was not found in the CSV file.\", file=sys.stderr)\n",
        "                return\n",
        "\n",
        "            # CRITICAL FIX: Convert the 'sno' column to a string and strip whitespace\n",
        "            chunk['sno'] = chunk['sno'].astype(str).str.strip()\n",
        "\n",
        "            # Filter the chunk to remove the specified stations\n",
        "            cleaned_chunk = chunk[~chunk['sno'].isin(stations_to_delete)]\n",
        "\n",
        "            # Write the cleaned chunk to the new file\n",
        "            # If it's the first chunk, write with header. Otherwise, append without header.\n",
        "            cleaned_chunk.to_csv(\n",
        "                new_filepath,\n",
        "                index=False,\n",
        "                mode='w' if is_first_chunk else 'a',\n",
        "                header=is_first_chunk\n",
        "            )\n",
        "\n",
        "            total_rows_read += len(chunk)\n",
        "            total_rows_deleted += len(chunk) - len(cleaned_chunk)\n",
        "\n",
        "            print(f\"Processed batch {batch_number}. Total rows processed so far: {total_rows_read}. Rows deleted in this batch: {len(chunk) - len(cleaned_chunk)}.\")\n",
        "\n",
        "            is_first_chunk = False\n",
        "\n",
        "        print(\"\\n--- Operation Complete ---\")\n",
        "        print(f\"Total rows read: {total_rows_read}\")\n",
        "        print(f\"Total rows deleted: {total_rows_deleted}\")\n",
        "        print(f\"Final number of rows: {total_rows_read - total_rows_deleted}\")\n",
        "        print(f\"Cleaned data saved to: {new_filepath}\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: The file was not found at {filepath}. Please check the path.\", file=sys.stderr)\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\", file=sys.stderr)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    remove_stations_from_csv_in_batches()\n"
      ],
      "metadata": {
        "id": "ruaYX3528L80",
        "outputId": "9ded5b8d-869e-4944-be0c-6430373fdd0d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading data from: /content/drive/MyDrive/Youbike_Master_Project/YouBike_Demand_Forecast/data_clean/ubike_consolidated_data.csv\n",
            "Processed batch 1. Total rows processed so far: 12000000. Rows deleted in this batch: 23351.\n",
            "Processed batch 2. Total rows processed so far: 24000000. Rows deleted in this batch: 14982.\n",
            "Processed batch 3. Total rows processed so far: 36000000. Rows deleted in this batch: 28749.\n",
            "Processed batch 4. Total rows processed so far: 48000000. Rows deleted in this batch: 14387.\n",
            "Processed batch 5. Total rows processed so far: 60000000. Rows deleted in this batch: 14396.\n",
            "Processed batch 6. Total rows processed so far: 72000000. Rows deleted in this batch: 11846.\n",
            "Processed batch 7. Total rows processed so far: 84000000. Rows deleted in this batch: 9521.\n",
            "Processed batch 8. Total rows processed so far: 88611411. Rows deleted in this batch: 2298.\n",
            "\n",
            "--- Operation Complete ---\n",
            "Total rows read: 88611411\n",
            "Total rows deleted: 119530\n",
            "Final number of rows: 88491881\n",
            "Cleaned data saved to: /content/drive/MyDrive/Youbike_Master_Project/YouBike_Demand_Forecast/data_clean/ubike_consolidated_data_cleaned.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import sys\n",
        "from google.colab import drive\n",
        "\n",
        "print(\"Connecting to Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the directory and file path as provided\n",
        "CLEAN_DATA_DIR = '/content/drive/MyDrive/Youbike_Master_Project/YouBike_Demand_Forecast/data_clean/'\n",
        "file_to_analyze = 'ubike_consolidated_data_cleaned.csv'\n",
        "filepath = os.path.join(CLEAN_DATA_DIR, file_to_analyze)\n",
        "\n",
        "# Define the output file path to avoid overwriting the original\n",
        "output_file = 'ubike_consolidated_data_no_sna.csv'\n",
        "output_filepath = os.path.join(CLEAN_DATA_DIR, output_file)\n",
        "\n",
        "def drop_column_from_csv_in_batches():\n",
        "    \"\"\"\n",
        "    Reads a large CSV file in batches, removes a specified column,\n",
        "    and saves the result to a new file.\n",
        "    \"\"\"\n",
        "    print(f\"Reading data from: {filepath}\")\n",
        "\n",
        "    try:\n",
        "        if not os.path.exists(filepath):\n",
        "            print(f\"Error: The input file was not found at {filepath}. Please check the path.\", file=sys.stderr)\n",
        "            return\n",
        "\n",
        "        batch_size = 12000000\n",
        "        chunk_iterator = pd.read_csv(filepath, chunksize=batch_size, iterator=True)\n",
        "\n",
        "        is_first_chunk = True\n",
        "        total_rows_processed = 0\n",
        "\n",
        "        for batch_number, chunk in enumerate(chunk_iterator, 1):\n",
        "            if is_first_chunk and 'sna' not in chunk.columns:\n",
        "                print(\"Error: The 'sna' column was not found in the CSV file.\", file=sys.stderr)\n",
        "                return\n",
        "\n",
        "            # Drop the 'sna' column from the current chunk\n",
        "            chunk_without_sna = chunk.drop(columns=['sna'])\n",
        "\n",
        "            # Write the processed chunk to the new file\n",
        "            chunk_without_sna.to_csv(\n",
        "                output_filepath,\n",
        "                index=False,\n",
        "                mode='w' if is_first_chunk else 'a',\n",
        "                header=is_first_chunk\n",
        "            )\n",
        "\n",
        "            total_rows_processed += len(chunk)\n",
        "\n",
        "            print(f\"Processed batch {batch_number}. Total rows processed so far: {total_rows_processed}.\")\n",
        "\n",
        "            is_first_chunk = False\n",
        "\n",
        "        print(\"\\n--- Operation Complete ---\")\n",
        "        print(f\"Total rows processed: {total_rows_processed}\")\n",
        "        print(f\"Cleaned data saved to: {output_filepath}\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: The file was not found at {filepath}. Please check the path.\", file=sys.stderr)\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\", file=sys.stderr)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    drop_column_from_csv_in_batches()\n"
      ],
      "metadata": {
        "id": "pvzMWLi-O-3m",
        "outputId": "3ec012db-3257-4459-b4b7-4bfbaa67641b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connecting to Google Drive...\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Reading data from: /content/drive/MyDrive/Youbike_Master_Project/YouBike_Demand_Forecast/data_clean/ubike_consolidated_data_cleaned.csv\n",
            "Processed batch 1. Total rows processed so far: 12000000.\n",
            "Processed batch 2. Total rows processed so far: 24000000.\n",
            "Processed batch 3. Total rows processed so far: 36000000.\n",
            "Processed batch 4. Total rows processed so far: 48000000.\n",
            "Processed batch 5. Total rows processed so far: 60000000.\n",
            "Processed batch 6. Total rows processed so far: 72000000.\n",
            "Processed batch 7. Total rows processed so far: 84000000.\n",
            "Processed batch 8. Total rows processed so far: 88491881.\n",
            "\n",
            "--- Operation Complete ---\n",
            "Total rows processed: 88491881\n",
            "Cleaned data saved to: /content/drive/MyDrive/Youbike_Master_Project/YouBike_Demand_Forecast/data_clean/ubike_consolidated_data_no_sna.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import sys\n",
        "from google.colab import drive\n",
        "\n",
        "print(\"Connecting to Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the directory and file path\n",
        "CLEAN_DATA_DIR = '/content/drive/MyDrive/Youbike_Master_Project/YouBike_Demand_Forecast/data_clean/'\n",
        "file_to_view = 'ubike_consolidated_data_no_sna.csv'\n",
        "filepath = os.path.join(CLEAN_DATA_DIR, file_to_view)\n",
        "\n",
        "def show_csv_head():\n",
        "    \"\"\"\n",
        "    Loads and displays the first few rows of the specified CSV file.\n",
        "    \"\"\"\n",
        "    print(f\"Loading and displaying the head of: {filepath}\")\n",
        "\n",
        "    try:\n",
        "        if not os.path.exists(filepath):\n",
        "            print(f\"Error: The file was not found at {filepath}. Please check the path.\", file=sys.stderr)\n",
        "            return\n",
        "\n",
        "        # Read only the first 5 rows to save memory and time\n",
        "        df_head = pd.read_csv(filepath, nrows=5)\n",
        "\n",
        "        print(\"\\n--- File Head ---\")\n",
        "        print(df_head)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\", file=sys.stderr)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    show_csv_head()\n"
      ],
      "metadata": {
        "id": "tD5yeI4OP-Kk",
        "outputId": "9d1e0e2c-271c-46de-cf56-fb3997975ab1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connecting to Google Drive...\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Loading and displaying the head of: /content/drive/MyDrive/Youbike_Master_Project/YouBike_Demand_Forecast/data_clean/ubike_consolidated_data_no_sna.csv\n",
            "\n",
            "--- File Head ---\n",
            "                  mday        sno  total  available_rent_bikes  \\\n",
            "0  2024-05-04 00:00:00  500101001   28.0                   6.0   \n",
            "1  2024-05-04 00:10:00  500101001   28.0                   6.0   \n",
            "2  2024-05-04 00:20:00  500101001   28.0                   3.0   \n",
            "3  2024-05-04 00:30:00  500101001   28.0                   0.0   \n",
            "4  2024-05-04 00:40:00  500101001   28.0                   1.0   \n",
            "\n",
            "   available_return_bikes       lat       lng     sareaen  \\\n",
            "0                    22.0  25.02605  121.5436  Daan Dist.   \n",
            "1                    22.0  25.02605  121.5436  Daan Dist.   \n",
            "2                    25.0  25.02605  121.5436  Daan Dist.   \n",
            "3                    28.0  25.02605  121.5436  Daan Dist.   \n",
            "4                    27.0  25.02605  121.5436  Daan Dist.   \n",
            "\n",
            "                       snaen  \n",
            "0  MRT Technology Bldg. Sta.  \n",
            "1  MRT Technology Bldg. Sta.  \n",
            "2  MRT Technology Bldg. Sta.  \n",
            "3  MRT Technology Bldg. Sta.  \n",
            "4  MRT Technology Bldg. Sta.  \n"
          ]
        }
      ]
    }
  ]
}