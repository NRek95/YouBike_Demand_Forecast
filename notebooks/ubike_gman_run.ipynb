{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "818682bb954c493cae203ece27035494": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_eff2fe893a854b6197420d92ddfff8aa",
              "IPY_MODEL_fbadf302875947a89f4de0d813eb8dae",
              "IPY_MODEL_88306335c71241b7949cce5c1d395f68"
            ],
            "layout": "IPY_MODEL_d877cc4d53f04124856497b13dbb3d79"
          }
        },
        "eff2fe893a854b6197420d92ddfff8aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e3d0056adc4d4483b3e65c87c286dde4",
            "placeholder": "​",
            "style": "IPY_MODEL_75634f0512f84a688325c5848a80b0ef",
            "value": "Computing transition probabilities: 100%"
          }
        },
        "fbadf302875947a89f4de0d813eb8dae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_782d12a3fab94bf2b7047e36efd08ebf",
            "max": 105,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5e87f72c42374e44b374d2191f17e610",
            "value": 105
          }
        },
        "88306335c71241b7949cce5c1d395f68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5ff4093c38ad456dbf38a359f43bc908",
            "placeholder": "​",
            "style": "IPY_MODEL_c0db693062e44dc2905ee1540e09c33c",
            "value": " 105/105 [00:01&lt;00:00, 61.50it/s]"
          }
        },
        "d877cc4d53f04124856497b13dbb3d79": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3d0056adc4d4483b3e65c87c286dde4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75634f0512f84a688325c5848a80b0ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "782d12a3fab94bf2b7047e36efd08ebf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e87f72c42374e44b374d2191f17e610": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5ff4093c38ad456dbf38a359f43bc908": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c0db693062e44dc2905ee1540e09c33c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import os\n",
        "import time\n",
        "import math\n",
        "from google.colab import drive\n",
        "\n",
        "# --- 1. Configuration ---\n",
        "# Input files\n",
        "FILE_PATH = \"/content/drive/MyDrive/Youbike_Master_Project/YouBike_Demand_Forecast/data/gongguan_model_ready_features.parquet.gz\"\n",
        "DISTANCE_CSV_PATH = \"/content/drive/MyDrive/Youbike_Master_Project/YouBike_Demand_Forecast/data/youbike_distances.csv\"\n",
        "\n",
        "# Directory for processed GMAN data\n",
        "GMAN_DATA_DIR = \"/content/drive/MyDrive/Youbike_Master_Project/GMAN_data/\"\n",
        "\n",
        "# Output files that will be created\n",
        "TRAFFIC_FILE = os.path.join(GMAN_DATA_DIR, \"youbike_traffic_full.h5\")\n",
        "SE_FILE = os.path.join(GMAN_DATA_DIR, \"SE_gman_node2vec.txt\") # This is the final SE file\n",
        "\n",
        "# Model saving directories\n",
        "MODEL_DIR = \"/content/drive/MyDrive/Youbike_Master_Project/GMAN_Checkpoints_TF2/\"\n",
        "LOG_DIR = \"/content/drive/MyDrive/Youbike_Master_Project/GMAN_logs_TF2/\"\n",
        "\n",
        "# --- 2. Mount Drive and Create Directories ---\n",
        "drive.mount('/content/drive')\n",
        "os.makedirs(GMAN_DATA_DIR, exist_ok=True)\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "os.makedirs(LOG_DIR, exist_ok=True)\n",
        "print(\"✅ Google Drive mounted and directories are ready.\")\n",
        "print(f\"TensorFlow Version: {tf.__version__}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Tc3x_tWy3x6",
        "outputId": "7b51c8f8-bf9f-4cc9-9618-79d11e2de9c5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "✅ Google Drive mounted and directories are ready.\n",
            "TensorFlow Version: 2.19.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# CELL 2 (FINAL, FULLY UPDATED): DATA PREPARATION PIPELINE\n",
        "# ==============================================================================\n",
        "!pip install -q node2vec\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "from node2vec import Node2Vec\n",
        "\n",
        "# --- Step 1: Create the Traffic File (Optimized to load only 3 columns) ---\n",
        "print(\"--- Step 1: Creating the traffic matrix from the full dataset... ---\")\n",
        "traffic_columns = ['time', 'sno', 'occupancy_ratio']\n",
        "data = pd.read_parquet(FILE_PATH, columns=traffic_columns)\n",
        "data['time'] = pd.to_datetime(data['time'])\n",
        "traffic_df = data.pivot(index='time', columns='sno', values='occupancy_ratio')\n",
        "traffic_df = traffic_df.resample('10T').mean().ffill()\n",
        "traffic_df.to_hdf(TRAFFIC_FILE, key='df', mode='w')\n",
        "print(f\"✅ Traffic data shape: {traffic_df.shape}. Saved to: {TRAFFIC_FILE}\")\n",
        "\n",
        "# --- Step 2: Create the Normalized Adjacency Matrix from the distance CSV ---\n",
        "print(\"\\n--- Step 2: Creating the normalized adjacency matrix... ---\")\n",
        "# This creates an intermediate file that node2vec will use\n",
        "normalized_adj_path = os.path.join(GMAN_DATA_DIR, 'SE_gman_normalized.npy')\n",
        "\n",
        "try:\n",
        "    distance_df = pd.read_csv(DISTANCE_CSV_PATH)\n",
        "    distance_matrix_df = distance_df.pivot_table(values='distance', index='from_station_id', columns='to_station_id')\n",
        "    dist_mx = distance_matrix_df.to_numpy()\n",
        "\n",
        "    distances = dist_mx[~np.isclose(dist_mx, 0)]\n",
        "    std = distances.std()\n",
        "    adj_mx = np.exp(-np.square(dist_mx / std))\n",
        "\n",
        "    np.save(normalized_adj_path, adj_mx)\n",
        "    print(f\"✅ Normalized adjacency matrix created at: {normalized_adj_path}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"❌ ERROR: 'youbike_distances.csv' not found. Please ensure it is at the correct path in the DISTANCE_CSV_PATH variable.\")\n",
        "    raise\n",
        "\n",
        "# --- Step 3: Run Node2Vec on the Adjacency Matrix with Paper's Hyperparameters ---\n",
        "print(\"\\n--- Step 3: Running Node2Vec to learn final spatial embeddings... ---\")\n",
        "graph = nx.from_numpy_array(adj_mx)\n",
        "\n",
        "# Use the hyperparameters from the original generateSE.py\n",
        "node2vec = Node2Vec(\n",
        "    graph,\n",
        "    dimensions=64,\n",
        "    walk_length=80,    # Updated\n",
        "    num_walks=100,     # Updated\n",
        "    p=2,               # Updated\n",
        "    q=1,               # Updated\n",
        "    workers=4\n",
        ")\n",
        "\n",
        "# Fit the model with the specified number of training iterations (epochs)\n",
        "model = node2vec.fit(window=10, min_count=1, batch_words=4, epochs=1000) # Updated\n",
        "\n",
        "# Save the final embeddings to the path specified in Cell 1\n",
        "model.wv.save_word2vec_format(SE_FILE)\n",
        "print(f\"✅ Node2Vec spatial embeddings saved to: {SE_FILE}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253,
          "referenced_widgets": [
            "818682bb954c493cae203ece27035494",
            "eff2fe893a854b6197420d92ddfff8aa",
            "fbadf302875947a89f4de0d813eb8dae",
            "88306335c71241b7949cce5c1d395f68",
            "d877cc4d53f04124856497b13dbb3d79",
            "e3d0056adc4d4483b3e65c87c286dde4",
            "75634f0512f84a688325c5848a80b0ef",
            "782d12a3fab94bf2b7047e36efd08ebf",
            "5e87f72c42374e44b374d2191f17e610",
            "5ff4093c38ad456dbf38a359f43bc908",
            "c0db693062e44dc2905ee1540e09c33c"
          ]
        },
        "id": "ULgq2QaLBlCc",
        "outputId": "f6025f6e-276e-4a38-9fc1-de09951bc467"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Step 1: Creating the traffic matrix from the full dataset... ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1995711251.py:16: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
            "  traffic_df = traffic_df.resample('10T').mean().ffill()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Traffic data shape: (59760, 105). Saved to: /content/drive/MyDrive/Youbike_Master_Project/GMAN_data/youbike_traffic_full.h5\n",
            "\n",
            "--- Step 2: Creating the normalized adjacency matrix... ---\n",
            "✅ Normalized adjacency matrix created at: /content/drive/MyDrive/Youbike_Master_Project/GMAN_data/SE_gman_normalized.npy\n",
            "\n",
            "--- Step 3: Running Node2Vec to learn final spatial embeddings... ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Computing transition probabilities:   0%|          | 0/105 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "818682bb954c493cae203ece27035494"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Node2Vec spatial embeddings saved to: /content/drive/MyDrive/Youbike_Master_Project/GMAN_data/SE_gman_node2vec.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Utility Functions ---\n",
        "def log_string(log, string):\n",
        "    log.write(string + '\\n'); log.flush(); print(string)\n",
        "\n",
        "def metric(pred, label):\n",
        "    mask = np.not_equal(label, 0).astype(np.float32)\n",
        "    mask /= np.mean(mask)\n",
        "    mae = np.mean(np.abs(pred - label) * mask)\n",
        "    rmse = np.sqrt(np.mean((pred - label)**2 * mask))\n",
        "    return mae, rmse\n",
        "\n",
        "def seq2instance(data, P, Q):\n",
        "    num_step, dims = data.shape\n",
        "    num_sample = num_step - P - Q + 1\n",
        "    x = np.zeros((num_sample, P, dims)); y = np.zeros((num_sample, Q, dims))\n",
        "    for i in range(num_sample):\n",
        "        x[i] = data[i: i + P]; y[i] = data[i + P: i + P + Q]\n",
        "    return x, y\n",
        "\n",
        "def load_data(args):\n",
        "    df = pd.read_hdf(args.traffic_file)\n",
        "\n",
        "    # --- THIS BLOCK IS MODIFIED ---\n",
        "    # Define the cutoff dates for training, validation, and testing\n",
        "    train_cutoff = '2025-03-04 00:00:00'\n",
        "    val_cutoff = '2025-05-04 00:00:00'\n",
        "\n",
        "    # Split the main traffic dataframe by date\n",
        "    train_df = df[df.index < train_cutoff]\n",
        "    val_df = df[(df.index >= train_cutoff) & (df.index < val_cutoff)]\n",
        "    test_df = df[df.index >= val_cutoff]\n",
        "\n",
        "    # Convert the sliced dataframes to numpy arrays\n",
        "    train = train_df.values\n",
        "    val = val_df.values\n",
        "    test = test_df.values\n",
        "    # --- END OF MODIFICATION ---\n",
        "\n",
        "    # The rest of the traffic data processing remains the same\n",
        "    trainX, trainY = seq2instance(train, args.P, args.Q)\n",
        "    valX, valY = seq2instance(val, args.P, args.Q)\n",
        "    testX, testY = seq2instance(test, args.P, args.Q)\n",
        "    mean, std = np.mean(trainX), np.std(trainX)\n",
        "    trainX = (trainX - mean) / std; valX = (valX - mean) / std; testX = (testX - mean) / std\n",
        "\n",
        "    # Loading the SE file remains the same\n",
        "    with open(args.SE_file, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "        temp = lines[0].split(' ')\n",
        "        N, dims = int(temp[0]), int(temp[1])\n",
        "        SE = np.zeros((N, dims), dtype=np.float32)\n",
        "        for line in lines[1:]:\n",
        "            temp = line.strip().split(' ')\n",
        "            index = int(temp[0])\n",
        "            SE[index] = [float(val) for val in temp[1:]]\n",
        "\n",
        "    # --- THIS BLOCK FOR TEMPORAL FEATURES IS ALSO MODIFIED ---\n",
        "    # Create temporal features for the entire dataset first\n",
        "    Time = df.index\n",
        "    dayofweek = np.reshape(Time.weekday, (-1, 1))\n",
        "    timeofday = (Time.hour * 60 + Time.minute) // 10\n",
        "    timeofday = np.reshape(timeofday, (-1, 1))\n",
        "    Time = np.concatenate((dayofweek, timeofday), axis=-1)\n",
        "\n",
        "    # Now, split the temporal features using the same date cutoffs\n",
        "    train_time = Time[:len(train)]\n",
        "    val_time = Time[len(train) : len(train) + len(val)]\n",
        "    test_time = Time[len(train) + len(val):]\n",
        "\n",
        "    # The rest of the temporal feature processing remains the same\n",
        "    trainTE = np.concatenate(seq2instance(train_time, args.P, args.Q), axis=1).astype(np.int32)\n",
        "    valTE = np.concatenate(seq2instance(val_time, args.P, args.Q), axis=1).astype(np.int32)\n",
        "    testTE = np.concatenate(seq2instance(test_time, args.P, args.Q), axis=1).astype(np.int32)\n",
        "    # --- END OF MODIFICATION ---\n",
        "\n",
        "    return trainX, trainTE, trainY, valX, valTE, valY, testX, testTE, testY, SE, mean, std\n",
        "\n",
        "# --- Keras Layers for GMAN ---\n",
        "# --- THIS CLASS IS UPDATED TO INCLUDE BATCH NORMALIZATION ---\n",
        "\n",
        "class FullyConnected(layers.Layer):\n",
        "    def __init__(self, units, activations, use_bias=True):\n",
        "        super(FullyConnected, self).__init__()\n",
        "        if isinstance(units, int):\n",
        "            units = [units]\n",
        "            activations = [activations]\n",
        "\n",
        "        self.convs = []\n",
        "        self.batch_norms = []\n",
        "        self.activations = activations # Store the activations list\n",
        "\n",
        "        for num_unit in units:\n",
        "            self.convs.append(layers.Conv2D(num_unit, kernel_size=1, use_bias=use_bias))\n",
        "            self.batch_norms.append(layers.BatchNormalization())\n",
        "\n",
        "    def call(self, x, training=None):\n",
        "        # Iterate through the layers, applying activations as specified\n",
        "        for i in range(len(self.convs)):\n",
        "            x = self.convs[i](x)\n",
        "            x = self.batch_norms[i](x, training=training)\n",
        "\n",
        "            # --- THIS IS THE FIX ---\n",
        "            # Only apply an activation if it's not None\n",
        "            if self.activations[i] is not None:\n",
        "                x = self.activations[i](x)\n",
        "            # --- END OF FIX ---\n",
        "\n",
        "        return x\n",
        "\n",
        "class STEmbedding(layers.Layer):\n",
        "    def __init__(self, D):\n",
        "        super(STEmbedding, self).__init__()\n",
        "        self.FC_se = FullyConnected([D, D], [tf.nn.relu, None])\n",
        "        self.day_embed = layers.Embedding(7, D)\n",
        "        self.time_embed = layers.Embedding(24 * 6, D) # 144 10-minute intervals in a day\n",
        "\n",
        "    def call(self, SE, TE):\n",
        "        # 1. Process the Spatial Embedding (SE)\n",
        "        # Shape: (num_stations, 2) -> (1, 1, num_stations, D)\n",
        "        SE = self.FC_se(tf.expand_dims(tf.expand_dims(SE, 0), 0))\n",
        "\n",
        "        # 2. Process the Temporal Embeddings (TE)\n",
        "        # Shape: (batch_size, seq_len, D)\n",
        "        day_TE = self.day_embed(TE[..., 0])\n",
        "        time_TE = self.time_embed(TE[..., 1])\n",
        "\n",
        "        # --- THIS IS THE FIX ---\n",
        "        # 3. Reshape TE to make it broadcastable with SE\n",
        "        # We add a dimension for the number of stations.\n",
        "        # Shape: (batch_size, seq_len, D) -> (batch_size, seq_len, 1, D)\n",
        "        day_TE = tf.expand_dims(day_TE, axis=2)\n",
        "        time_TE = tf.expand_dims(time_TE, axis=2)\n",
        "        # --- END OF FIX ---\n",
        "\n",
        "        # 4. Add the embeddings together. Broadcasting handles the rest.\n",
        "        # (1, 1, num_stations, D) + (batch, seq, 1, D) -> (batch, seq, num_stations, D)\n",
        "        return SE + day_TE + time_TE\n",
        "\n",
        "\n",
        "class SpatialAttention(layers.Layer):\n",
        "    def __init__(self, K, d):\n",
        "        super(SpatialAttention, self).__init__()\n",
        "        self.K, self.d = K, d\n",
        "        self.D = K * d\n",
        "        self.FC_q = FullyConnected(self.D, tf.nn.relu)\n",
        "        self.FC_k = FullyConnected(self.D, tf.nn.relu)\n",
        "        self.FC_v = FullyConnected(self.D, tf.nn.relu) # For the value tensor\n",
        "        self.FC_o = FullyConnected(self.D, tf.nn.relu)\n",
        "\n",
        "    def call(self, X, STE):\n",
        "        # Concatenate X and STE first\n",
        "        X_with_STE = tf.concat([X, STE], axis=-1)\n",
        "\n",
        "        query = self.FC_q(X_with_STE)\n",
        "        key = self.FC_k(X_with_STE)\n",
        "\n",
        "        # --- THIS IS THE FINAL CORRECTION ---\n",
        "        # The 'value' must also be derived from the concatenated tensor\n",
        "        value = self.FC_v(X_with_STE)\n",
        "        # --- END OF CORRECTION ---\n",
        "\n",
        "        query = tf.concat(tf.split(query, self.K, axis=-1), axis=0)\n",
        "        key = tf.concat(tf.split(key, self.K, axis=-1), axis=0)\n",
        "        value = tf.concat(tf.split(value, self.K, axis=-1), axis=0)\n",
        "\n",
        "        attention = tf.matmul(query, key, transpose_b=True) / (self.d ** 0.5)\n",
        "        attention = tf.nn.softmax(attention)\n",
        "\n",
        "        X_ = tf.matmul(attention, value)\n",
        "        X_ = tf.concat(tf.split(X_, self.K, axis=0), axis=-1)\n",
        "\n",
        "        return self.FC_o(X_)\n",
        "\n",
        "class TemporalAttention(layers.Layer):\n",
        "    def __init__(self, K, d):\n",
        "        super(TemporalAttention, self).__init__()\n",
        "        self.K, self.d = K, d\n",
        "        self.D = K * d\n",
        "        self.FC_q = FullyConnected(self.D, tf.nn.relu)\n",
        "        self.FC_k = FullyConnected(self.D, tf.nn.relu)\n",
        "        self.FC_v = FullyConnected(self.D, tf.nn.relu) # For the value tensor\n",
        "        self.FC_o = FullyConnected(self.D, tf.nn.relu)\n",
        "\n",
        "    def call(self, X, STE):\n",
        "        # Concatenate X and STE first\n",
        "        X_with_STE = tf.concat([X, STE], axis=-1)\n",
        "\n",
        "        query = self.FC_q(X_with_STE)\n",
        "        key = self.FC_k(X_with_STE)\n",
        "\n",
        "        # --- THIS IS THE FINAL CORRECTION ---\n",
        "        # The 'value' must also be derived from the concatenated tensor\n",
        "        value = self.FC_v(X_with_STE)\n",
        "        # --- END OF CORRECTION ---\n",
        "\n",
        "        query = tf.concat(tf.split(query, self.K, axis=-1), axis=0)\n",
        "        key = tf.concat(tf.split(key, self.K, axis=-1), axis=0)\n",
        "        value = tf.concat(tf.split(value, self.K, axis=-1), axis=0)\n",
        "\n",
        "        query = tf.transpose(query, (0, 2, 1, 3))\n",
        "        key = tf.transpose(key, (0, 2, 3, 1))\n",
        "        value = tf.transpose(value, (0, 2, 1, 3))\n",
        "\n",
        "        attention = tf.matmul(query, key) / (self.d ** 0.5)\n",
        "        attention = tf.nn.softmax(attention)\n",
        "\n",
        "        X_ = tf.matmul(attention, value)\n",
        "        X_ = tf.transpose(X_, (0, 2, 1, 3))\n",
        "        X_ = tf.concat(tf.split(X_, self.K, axis=0), axis=-1)\n",
        "\n",
        "        return self.FC_o(X_)\n",
        "\n",
        "class GatedFusion(layers.Layer):\n",
        "    def __init__(self, D):\n",
        "        super(GatedFusion, self).__init__()\n",
        "        # --- THIS BLOCK IS MODIFIED ---\n",
        "        # Create two separate FullyConnected layers, one for spatial and one for temporal\n",
        "        self.FC_xs = FullyConnected(D, activations=None, use_bias=False)\n",
        "        self.FC_xt = FullyConnected(D, activations=None, use_bias=True)\n",
        "        # --- END OF MODIFICATION ---\n",
        "\n",
        "    def call(self, X_s, X_t):\n",
        "        # --- THIS BLOCK IS MODIFIED TO MATCH THE ORIGINAL CODE ---\n",
        "        # Project spatial and temporal outputs separately\n",
        "        XS = self.FC_xs(X_s)\n",
        "        XT = self.FC_xt(X_t)\n",
        "\n",
        "        # Add the projected outputs and apply the sigmoid gate\n",
        "        z = tf.nn.sigmoid(tf.add(XS, XT))\n",
        "\n",
        "        # Fuse the original (non-projected) attention outputs\n",
        "        H = z * X_s + (1 - z) * X_t\n",
        "\n",
        "        # The final projection layer from the original code's GatedFusion function\n",
        "        # is now implicitly handled by the subsequent layers in the STAttBlock's residual connection\n",
        "        return H\n",
        "\n",
        "# ==============================================================================\n",
        "# CORRECTED ST-ATTENTION BLOCK (Replace the old class with this)\n",
        "# ==============================================================================\n",
        "\n",
        "class STAttBlock(layers.Layer):\n",
        "    def __init__(self, K, d):\n",
        "        super(STAttBlock, self).__init__()\n",
        "        self.spatial_attention = SpatialAttention(K, d)\n",
        "        self.temporal_attention = TemporalAttention(K, d)\n",
        "        self.gated_fusion = GatedFusion(K * d)\n",
        "\n",
        "    def call(self, X, STE):\n",
        "        X_s = self.spatial_attention(X, STE)\n",
        "        X_t = self.temporal_attention(X, STE)\n",
        "\n",
        "        # --- THIS IS THE FINAL FIX ---\n",
        "        # The output of the gated fusion is computed\n",
        "        H = self.gated_fusion(X_s, X_t)\n",
        "        # The residual connection is added: input + output\n",
        "        return X + H\n",
        "        # --- END OF FIX ---\n",
        "\n",
        "class TransformAttention(layers.Layer):\n",
        "    def __init__(self, K, d):\n",
        "        super(TransformAttention, self).__init__()\n",
        "        self.K, self.d = K, d\n",
        "        self.D = K * d\n",
        "        self.FC_q = FullyConnected(self.D, tf.nn.relu)\n",
        "        self.FC_k = FullyConnected(self.D, tf.nn.relu)\n",
        "        self.FC_v = FullyConnected(self.D, tf.nn.relu)\n",
        "        self.FC_o = FullyConnected(self.D, tf.nn.relu)\n",
        "    def call(self, X, STE_P, STE_Q):\n",
        "        query = self.FC_q(STE_Q); key = self.FC_k(STE_P); value = self.FC_v(X)\n",
        "        query = tf.concat(tf.split(query, self.K, axis=-1), axis=0)\n",
        "        key = tf.concat(tf.split(key, self.K, axis=-1), axis=0)\n",
        "        value = tf.concat(tf.split(value, self.K, axis=-1), axis=0)\n",
        "        query = tf.transpose(query, (0, 2, 1, 3))\n",
        "        key = tf.transpose(key, (0, 2, 3, 1))\n",
        "        value = tf.transpose(value, (0, 2, 1, 3))\n",
        "        attention = tf.matmul(query, key) / (self.d ** 0.5)\n",
        "        attention = tf.nn.softmax(attention)\n",
        "        X_ = tf.matmul(attention, value)\n",
        "        X_ = tf.transpose(X_, (0, 2, 1, 3))\n",
        "        X_ = tf.concat(tf.split(X_, self.K, axis=0), axis=-1)\n",
        "        return self.FC_o(X_)\n",
        "\n",
        "class GMAN(keras.Model):\n",
        "    def __init__(self, L, K, d, P, Q):\n",
        "        super(GMAN, self).__init__()\n",
        "        self.L, self.K, self.d, self.P, self.Q = L, K, d, P, Q\n",
        "        self.D = K * d\n",
        "        self.FC_input = FullyConnected([self.D, self.D], [tf.nn.relu, None])\n",
        "        self.STEmbedding = STEmbedding(self.D)\n",
        "        self.encoder = [STAttBlock(K, d) for _ in range(L)]\n",
        "        self.transform_attention = TransformAttention(K, d)\n",
        "        self.decoder = [STAttBlock(K, d) for _ in range(L)]\n",
        "        self.FC_output = FullyConnected([self.D, 1], [tf.nn.relu, None])\n",
        "    def call(self, X, SE, TE):\n",
        "        X = self.FC_input(tf.expand_dims(X, -1))\n",
        "        STE = self.STEmbedding(SE, TE)\n",
        "        STE_P, STE_Q = STE[:, :self.P], STE[:, self.P:]\n",
        "        for block in self.encoder: X = block(X, STE_P)\n",
        "        X = self.transform_attention(X, STE_P, STE_Q)\n",
        "        for block in self.decoder: X = block(X, STE_Q)\n",
        "        return tf.squeeze(self.FC_output(X), -1)\n",
        "\n",
        "print(\"✅ GMAN model and utility functions defined for TensorFlow 2.x.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OkzFi63LzCP6",
        "outputId": "ed8b4d95-8607-4ca7-ffd8-d5c4b42c8fe9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ GMAN model and utility functions defined for TensorFlow 2.x.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Final Cell (UPDATED): GMAN Training with Original Hyperparameters & Scheduler\n",
        "# ==============================================================================\n",
        "from tensorflow.keras import mixed_precision\n",
        "mixed_precision.set_global_policy('mixed_float16')\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# --- 1. Set Hyperparameters (Aligned with original train.py) ---\n",
        "class Args:\n",
        "    P = 6; Q = 6; L = 3; K = 8; d = 8\n",
        "    # Using date-based split, so ratios are not needed here\n",
        "    batch_size = 32          # UPDATED to match original paper's default\n",
        "    max_epoch = 50\n",
        "    patience = 10\n",
        "    learning_rate = 0.001    # UPDATED to your requested value\n",
        "    decay_epoch = 5          # From original train.py\n",
        "    decay_rate = 0.7         # From original train.py\n",
        "    traffic_file = TRAFFIC_FILE\n",
        "    SE_file = SE_FILE\n",
        "    model_file = os.path.join(MODEL_DIR, \"gman_model_tf2.weights.h5\")\n",
        "\n",
        "args = Args()\n",
        "\n",
        "# --- 2. Load Data ---\n",
        "trainX, trainTE, trainY, valX, valTE, valY, testX, testTE, testY, SE, mean, std = load_data(args)\n",
        "print(f'Number of training samples: {trainX.shape[0]}')\n",
        "\n",
        "# --- 3. Build Model, Optimizer, and ExponentialDecay Scheduler ---\n",
        "model = GMAN(args.L, args.K, args.d, args.P, args.Q)\n",
        "\n",
        "# --- THIS BLOCK IS UPDATED ---\n",
        "# Create an ExponentialDecay learning rate scheduler\n",
        "num_train_steps_per_epoch = math.ceil(trainX.shape[0] / args.batch_size)\n",
        "decay_steps = args.decay_epoch * num_train_steps_per_epoch\n",
        "\n",
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    args.learning_rate,\n",
        "    decay_steps=decay_steps,\n",
        "    decay_rate=args.decay_rate,\n",
        "    staircase=True)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule, clipnorm=1.0)\n",
        "# --- END OF UPDATE ---\n",
        "\n",
        "loss_fn = tf.keras.losses.MeanAbsoluteError()\n",
        "\n",
        "# --- 4. Training Loop ---\n",
        "print(\"\\n--- Starting Model Training with Exponential Decay Scheduler ---\")\n",
        "wait = 0\n",
        "val_loss_min = np.inf\n",
        "\n",
        "# Add shuffling to the training dataset\n",
        "buffer_size = trainX.shape[0]\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((trainX, trainTE, trainY)).shuffle(buffer_size).batch(args.batch_size).cache().prefetch(tf.data.AUTOTUNE)\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((valX, valTE, valY)).batch(args.batch_size).cache().prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "for epoch in range(args.max_epoch):\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Training step (no changes needed here)\n",
        "    for step, (x_batch, te_batch, y_batch) in enumerate(train_dataset):\n",
        "        with tf.GradientTape() as tape:\n",
        "            pred = model(x_batch, SE, te_batch) * std + mean\n",
        "            loss = loss_fn(y_batch, pred)\n",
        "        grads = tape.gradient(loss, model.trainable_weights)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "\n",
        "    # Validation loop (no changes needed here)\n",
        "    val_loss = 0\n",
        "    num_val_batches = 0\n",
        "    for x_batch, te_batch, y_batch in val_dataset:\n",
        "        pred = model(x_batch, SE, te_batch, training=False) * std + mean\n",
        "        val_loss += loss_fn(y_batch, pred)\n",
        "        num_val_batches += 1\n",
        "    val_loss /= num_val_batches\n",
        "\n",
        "    # REPLACE WITH THIS LINE:\n",
        "    print(f'Epoch {epoch+1:2d}, Val MAE: {val_loss:.4f}, Time: {time.time() - start_time:.2f}s, LR: {lr_schedule(optimizer.iterations).numpy():.6f}')\n",
        "\n",
        "    # Early stopping (no changes needed here)\n",
        "    if val_loss < val_loss_min:\n",
        "        wait = 0\n",
        "        val_loss_min = val_loss\n",
        "        model.save_weights(args.model_file)\n",
        "        print(f'  > Val MAE improved to {val_loss_min:.4f}, saving model.')\n",
        "    else:\n",
        "        wait += 1\n",
        "\n",
        "    if wait >= args.patience:\n",
        "        print(f'  > Early stop at epoch {epoch+1}')\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwBBjuYczHxV",
        "outputId": "525be5b3-e24b-452f-b927-344b557c2eba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training samples: 43765\n",
            "\n",
            "--- Starting Model Training with Exponential Decay Scheduler ---\n",
            "Epoch  1, Val MAE: 0.0859, Time: 2648.69s, LR: 0.001000\n",
            "  > Val MAE improved to 0.0859, saving model.\n",
            "Epoch  2, Val MAE: 0.0811, Time: 2620.64s, LR: 0.001000\n",
            "  > Val MAE improved to 0.0811, saving model.\n",
            "Epoch  3, Val MAE: 0.0799, Time: 2612.75s, LR: 0.001000\n",
            "  > Val MAE improved to 0.0799, saving model.\n",
            "Epoch  4, Val MAE: 0.0796, Time: 2614.92s, LR: 0.001000\n",
            "  > Val MAE improved to 0.0796, saving model.\n",
            "Epoch  5, Val MAE: 0.0780, Time: 2610.21s, LR: 0.000700\n",
            "  > Val MAE improved to 0.0780, saving model.\n",
            "Epoch  6, Val MAE: 0.0764, Time: 2615.30s, LR: 0.000700\n",
            "  > Val MAE improved to 0.0764, saving model.\n",
            "Epoch  7, Val MAE: 0.0754, Time: 2612.87s, LR: 0.000700\n",
            "  > Val MAE improved to 0.0754, saving model.\n",
            "Epoch  8, Val MAE: 0.0751, Time: 2612.34s, LR: 0.000700\n",
            "  > Val MAE improved to 0.0751, saving model.\n",
            "Epoch  9, Val MAE: 0.0749, Time: 2611.39s, LR: 0.000700\n",
            "  > Val MAE improved to 0.0749, saving model.\n",
            "Epoch 10, Val MAE: 0.0744, Time: 2617.58s, LR: 0.000490\n",
            "  > Val MAE improved to 0.0744, saving model.\n",
            "Epoch 11, Val MAE: 0.0740, Time: 2611.62s, LR: 0.000490\n",
            "  > Val MAE improved to 0.0740, saving model.\n",
            "Epoch 12, Val MAE: 0.0744, Time: 2603.80s, LR: 0.000490\n",
            "Epoch 13, Val MAE: 0.0741, Time: 2601.73s, LR: 0.000490\n",
            "Epoch 14, Val MAE: 0.0739, Time: 2600.51s, LR: 0.000490\n",
            "  > Val MAE improved to 0.0739, saving model.\n",
            "Epoch 15, Val MAE: 0.0738, Time: 2603.56s, LR: 0.000343\n",
            "  > Val MAE improved to 0.0738, saving model.\n",
            "Epoch 16, Val MAE: 0.0736, Time: 2608.50s, LR: 0.000343\n",
            "  > Val MAE improved to 0.0736, saving model.\n",
            "Epoch 17, Val MAE: 0.0737, Time: 2617.54s, LR: 0.000343\n",
            "Epoch 18, Val MAE: 0.0735, Time: 2608.71s, LR: 0.000343\n",
            "  > Val MAE improved to 0.0735, saving model.\n",
            "Epoch 19, Val MAE: 0.0735, Time: 2615.30s, LR: 0.000343\n",
            "Epoch 20, Val MAE: 0.0736, Time: 2624.99s, LR: 0.000240\n",
            "Epoch 21, Val MAE: 0.0734, Time: 2622.45s, LR: 0.000240\n",
            "  > Val MAE improved to 0.0734, saving model.\n",
            "Epoch 22, Val MAE: 0.0734, Time: 2616.63s, LR: 0.000240\n",
            "  > Val MAE improved to 0.0734, saving model.\n",
            "Epoch 23, Val MAE: 0.0734, Time: 2627.96s, LR: 0.000240\n",
            "  > Val MAE improved to 0.0734, saving model.\n",
            "Epoch 24, Val MAE: 0.0735, Time: 2620.80s, LR: 0.000240\n",
            "Epoch 25, Val MAE: 0.0735, Time: 2618.77s, LR: 0.000168\n",
            "Epoch 26, Val MAE: 0.0734, Time: 2628.07s, LR: 0.000168\n",
            "Epoch 27, Val MAE: 0.0735, Time: 2621.28s, LR: 0.000168\n",
            "Epoch 28, Val MAE: 0.0735, Time: 2632.87s, LR: 0.000168\n",
            "Epoch 29, Val MAE: 0.0734, Time: 2623.54s, LR: 0.000168\n",
            "Epoch 30, Val MAE: 0.0735, Time: 2621.98s, LR: 0.000118\n",
            "Epoch 31, Val MAE: 0.0735, Time: 2626.07s, LR: 0.000118\n",
            "Epoch 32, Val MAE: 0.0736, Time: 2628.64s, LR: 0.000118\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 5. Final Evaluation on TEST SET ---\n",
        "import time # Ganz am Anfang deines Scripts importieren\n",
        "print(\"\\n\\n--- Final Detailed Evaluation on Test Set ---\")\n",
        "model.load_weights(args.model_file)\n",
        "\n",
        "# Generate predictions using a manual loop\n",
        "print(\"\\nGenerating predictions on the test set...\")\n",
        "# --- ZEITMESSUNG START ---\n",
        "start_time = time.time()\n",
        "\n",
        "test_preds = []\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((testX, testTE)).batch(args.batch_size)\n",
        "for x_batch, te_batch in test_dataset:\n",
        "    pred_batch = model(x_batch, SE, te_batch, training=False) * std + mean\n",
        "    test_preds.append(pred_batch.numpy())\n",
        "test_preds = np.concatenate(test_preds, axis=0)\n",
        "# --- ZEITMESSUNG ENDE ---\n",
        "end_time = time.time()\n",
        "prediction_duration = end_time - start_time\n",
        "print(\"✅ Test set predictions generated.\")\n",
        "\n",
        "print(\"✅ Test set predictions generated.\")\n",
        "# Gib die gemessene Zeit aus\n",
        "print(f\"   -> Prediction took {prediction_duration:.4f} seconds.\")\n",
        "\n",
        "# --- 5.1: Per-Horizon and Overall Metrics ---\n",
        "print('\\n--- Test Set Metrics for Each Horizon ---')\n",
        "for q in range(args.Q):\n",
        "    y_true_step = testY[:, q, :]\n",
        "    y_pred_step = test_preds[:, q, :]\n",
        "    mae = mean_absolute_error(y_true_step, y_pred_step)\n",
        "    mse = mean_squared_error(y_true_step, y_pred_step)\n",
        "    rmse = np.sqrt(mse)\n",
        "    r2 = r2_score(y_true_step, y_pred_step, multioutput='uniform_average')\n",
        "    print(f\"  Horizon t+{q+1}: MAE={mae:.4f}, MSE={mse:.4f}, RMSE={rmse:.4f}, R²={r2:.4f}\")\n",
        "\n",
        "print(\"\\n--- Overall Average Test Set Metrics (All Stations) ---\")\n",
        "y_true_flat = testY.ravel(); y_pred_flat = test_preds.ravel()\n",
        "print(f\"  Overall MAE:  {mean_absolute_error(y_true_flat, y_pred_flat):.4f}\")\n",
        "print(f\"  Overall MSE:  {mean_squared_error(y_true_flat, y_pred_flat):.4f}\")\n",
        "print(f\"  Overall RMSE: {np.sqrt(mean_squared_error(y_true_flat, y_pred_flat)):.4f}\")\n",
        "print(f\"  Overall R²:   {r2_score(y_true_flat, y_pred_flat):.4f}\")\n",
        "\n",
        "# --- 5.2: Identify and Analyze Best/Worst Stations ---\n",
        "print(\"\\n\\n--- Best vs. Worst Station Analysis ---\")\n",
        "num_stations = testY.shape[2]\n",
        "station_errors = [{'station_idx': i, 'mae': np.mean(np.abs(testY[:, :, i] - test_preds[:, :, i]))} for i in range(num_stations)]\n",
        "errors_df = pd.DataFrame(station_errors).sort_values('mae')\n",
        "best_stations_indices = errors_df.head(3)['station_idx'].tolist()\n",
        "worst_stations_indices = errors_df.tail(3)['station_idx'].tolist()\n",
        "\n",
        "def calculate_subset_metrics(indices):\n",
        "    y_true_subset = testY[:, :, indices].ravel()\n",
        "    y_pred_subset = test_preds[:, :, indices].ravel()\n",
        "    mae = mean_absolute_error(y_true_subset, y_pred_subset)\n",
        "    rmse = np.sqrt(mean_squared_error(y_true_subset, y_pred_subset))\n",
        "    r2 = r2_score(y_true_subset, y_pred_subset)\n",
        "    return mae, rmse, r2\n",
        "\n",
        "metrics_best_3 = calculate_subset_metrics(best_stations_indices)\n",
        "metrics_worst_3 = calculate_subset_metrics(worst_stations_indices)\n",
        "print(f\"\\n--- Metrics for the 3 BEST Stations (Indices: {best_stations_indices}) ---\")\n",
        "print(f\"  Best 3 MAE: {metrics_best_3[0]:.4f}, RMSE: {metrics_best_3[1]:.4f}, R²: {metrics_best_3[2]:.4f}\")\n",
        "print(f\"\\n--- Metrics for the 3 WORST Stations (Indices: {worst_stations_indices}) ---\")\n",
        "print(f\"  Worst 3 MAE: {metrics_worst_3[0]:.4f}, RMSE: {metrics_worst_3[1]:.4f}, R²: {metrics_worst_3[2]:.4f}\")\n",
        "\n",
        "# --- 5.3: Plot Forecasts for Best and Worst Stations ---\n",
        "horizon_to_plot = 5 # Corresponds to t+6 (0-indexed)\n",
        "\n",
        "def plot_station_forecast(station_idx, performance_type, color):\n",
        "    steps_to_plot = 288 # 2 days\n",
        "    start_index = np.random.randint(0, testY.shape[0] - steps_to_plot)\n",
        "    y_actual_slice = testY[start_index : start_index + steps_to_plot, horizon_to_plot, station_idx]\n",
        "    y_pred_slice = test_preds[start_index : start_index + steps_to_plot, horizon_to_plot, station_idx]\n",
        "    mae = errors_df[errors_df['station_idx'] == station_idx]['mae'].iloc[0]\n",
        "\n",
        "    # Filter out invalid typhoon data for plotting\n",
        "    valid_indices = y_actual_slice >= 0\n",
        "\n",
        "    plt.figure(figsize=(15, 6))\n",
        "    plt.plot(y_actual_slice[valid_indices], label='Actual Values', color='blue')\n",
        "    plt.plot(y_pred_slice[valid_indices], label='Predicted Values', color=color, linestyle='--')\n",
        "    plt.title(f'{performance_type} Station (Index: {station_idx}) | Horizon t+{horizon_to_plot+1} | Avg MAE: {mae:.4f}', fontsize=16)\n",
        "    plt.xlabel('Time Steps (10-minute intervals)'); plt.ylabel('Occupancy Ratio')\n",
        "    plt.legend(); plt.grid(True);\n",
        "    plt.show()\n",
        "\n",
        "print(f\"\\n--- Visualizing t+6 Forecasts for BEST Stations ---\")\n",
        "for station_idx in best_stations_indices:\n",
        "    plot_station_forecast(station_idx, \"BEST\", \"green\")\n",
        "\n",
        "print(f\"\\n--- Visualizing t+6 Forecasts for WORST Stations ---\")\n",
        "for station_idx in worst_stations_indices:\n",
        "    plot_station_forecast(station_idx, \"WORST\", \"red\")"
      ],
      "metadata": {
        "id": "zBpCJ7FwPgG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Final Cell: Comprehensive Evaluation on the TEST SET\n",
        "# ==============================================================================\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- 1. Generate Predictions on the TEST SET ---\n",
        "print(\"\\n--- Final Evaluation on Test Set ---\")\n",
        "model.load_weights(args.model_file)\n",
        "\n",
        "# Generate predictions using a manual loop\n",
        "print(\"\\nGenerating predictions on the test set...\")\n",
        "test_preds = []\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((testX, testTE)).batch(args.batch_size)\n",
        "for x_batch, te_batch in test_dataset:\n",
        "    pred_batch = model(x_batch, SE, te_batch, training=False) * std + mean\n",
        "    test_preds.append(pred_batch.numpy())\n",
        "test_preds = np.concatenate(test_preds, axis=0)\n",
        "print(\"✅ Test set predictions generated.\")\n",
        "\n",
        "\n",
        "# --- 2. Per-Horizon and Overall Metrics ---\n",
        "print('\\n--- Test Set Metrics for Each Horizon ---')\n",
        "for q in range(args.Q):\n",
        "    y_true_step = testY[:, q, :]\n",
        "    y_pred_step = test_preds[:, q, :]\n",
        "\n",
        "    mae = mean_absolute_error(y_true_step, y_pred_step)\n",
        "    mse = mean_squared_error(y_true_step, y_pred_step)\n",
        "    rmse = np.sqrt(mse)\n",
        "    r2 = r2_score(y_true_step, y_pred_step, multioutput='uniform_average')\n",
        "\n",
        "    print(f\"\\n--- Horizon t+{q+1} (+{(q+1)*10} min) ---\")\n",
        "    print(f\"MAE:  {mae:.4f}, MSE:  {mse:.4f}, RMSE: {rmse:.4f}, R²:   {r2:.4f}\")\n",
        "\n",
        "# Calculate overall average metrics by flattening the arrays\n",
        "print(\"\\n--- Overall Average Test Set Metrics (All Stations) ---\")\n",
        "y_true_flat = testY.ravel()\n",
        "y_pred_flat = test_preds.ravel()\n",
        "overall_mae = mean_absolute_error(y_true_flat, y_pred_flat)\n",
        "overall_mse = mean_squared_error(y_true_flat, y_pred_flat)\n",
        "overall_rmse = np.sqrt(overall_mse)\n",
        "overall_r2 = r2_score(y_true_flat, y_pred_flat)\n",
        "\n",
        "print(f\"Overall MAE:  {overall_mae:.4f}\")\n",
        "print(f\"Overall MSE:  {overall_mse:.4f}\")\n",
        "print(f\"Overall RMSE: {overall_rmse:.4f}\")\n",
        "print(f\"Overall R²:   {overall_r2:.4f}\")\n",
        "\n",
        "\n",
        "# --- 3. Identify and Analyze Best/Worst Performing Stations ---\n",
        "print(\"\\n\\n--- Best vs. Worst Station Analysis ---\")\n",
        "print(\"\\nCalculating Mean Absolute Error for each station...\")\n",
        "num_stations = testY.shape[2]\n",
        "station_errors = [{'station_idx': i, 'mae': np.mean(np.abs(testY[:, :, i] - test_preds[:, :, i]))} for i in range(num_stations)]\n",
        "\n",
        "# Sort stations by error to find the best and worst\n",
        "errors_df = pd.DataFrame(station_errors).sort_values('mae')\n",
        "best_stations_indices = errors_df.head(3)['station_idx'].tolist()\n",
        "worst_stations_indices = errors_df.tail(3)['station_idx'].tolist()\n",
        "print(\"✅ Best and worst stations identified.\")\n",
        "\n",
        "# Helper function to calculate metrics for a subset of stations\n",
        "def calculate_subset_metrics(indices):\n",
        "    y_true_subset = testY[:, :, indices].ravel()\n",
        "    y_pred_subset = test_preds[:, :, indices].ravel()\n",
        "    mae = mean_absolute_error(y_true_subset, y_pred_subset)\n",
        "    rmse = np.sqrt(mean_squared_error(y_true_subset, y_pred_subset))\n",
        "    r2 = r2_score(y_true_subset, y_pred_subset)\n",
        "    return mae, rmse, r2\n",
        "\n",
        "# Calculate and display metrics for the subsets\n",
        "metrics_best_3 = calculate_subset_metrics(best_stations_indices)\n",
        "metrics_worst_3 = calculate_subset_metrics(worst_stations_indices)\n",
        "\n",
        "print(\"\\n--- Average Metrics for the 3 BEST Performing Stations ---\")\n",
        "print(f\"(Station Indices: {best_stations_indices})\")\n",
        "print(f\"Best 3 MAE:   {metrics_best_3[0]:.4f}, RMSE: {metrics_best_3[1]:.4f}, R²: {metrics_best_3[2]:.4f}\")\n",
        "\n",
        "print(\"\\n--- Average Metrics for the 3 WORST Performing Stations ---\")\n",
        "print(f\"(Station Indices: {worst_stations_indices})\")\n",
        "print(f\"Worst 3 MAE:  {metrics_worst_3[0]:.4f}, RMSE: {metrics_worst_3[1]:.4f}, R²: {metrics_worst_3[2]:.4f}\")\n",
        "\n",
        "\n",
        "# --- 4. Plot Forecasts for Best and Worst Stations ---\n",
        "horizon_to_plot = 5 # Corresponds to t+6 (0-indexed)\n",
        "\n",
        "def plot_station_forecast(station_idx, performance_type, color):\n",
        "    # Select a random 2-day slice of the test data for plotting\n",
        "    steps_to_plot = 288\n",
        "    start_index = np.random.randint(0, testY.shape[0] - steps_to_plot)\n",
        "    y_actual_slice = testY[start_index : start_index + steps_to_plot, horizon_to_plot, station_idx]\n",
        "    y_pred_slice = test_preds[start_index : start_index + steps_to_plot, horizon_to_plot, station_idx]\n",
        "    mae = errors_df[errors_df['station_idx'] == station_idx]['mae'].iloc[0]\n",
        "\n",
        "    plt.figure(figsize=(15, 6))\n",
        "    plt.plot(y_actual_slice, label='Actual Values', color='blue')\n",
        "    plt.plot(y_pred_slice, label='Predicted Values', color=color, linestyle='--')\n",
        "    plt.title(f'{performance_type} Performing Station (Index: {station_idx}) | Avg MAE: {mae:.4f}', fontsize=16)\n",
        "    plt.xlabel(f'Time Steps (10-minute intervals, Horizon t+{horizon_to_plot+1})'); plt.ylabel('Occupancy Ratio')\n",
        "    plt.legend(); plt.grid(True); plt.ylim(0, 1)\n",
        "    plt.show()\n",
        "\n",
        "print(f\"\\n--- Visualizing t+6 Forecasts for BEST Stations ---\")\n",
        "for station_idx in best_stations_indices:\n",
        "    plot_station_forecast(station_idx, \"BEST\", \"green\")\n",
        "\n",
        "print(f\"\\n--- Visualizing t+6 Forecasts for WORST Stations ---\")\n",
        "for station_idx in worst_stations_indices:\n",
        "    plot_station_forecast(station_idx, \"WORST\", \"red\")"
      ],
      "metadata": {
        "id": "MxFpc7z22wri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the file path and cutoff date from your script\n",
        "FILE_PATH = \"/content/drive/MyDrive/Youbike_Master_Project/YouBike_Demand_Forecast/data/gongguan_model_ready_features.parquet.gz\"\n",
        "VALIDATION_CUTOFF_DATE = '2024-09-20'\n",
        "\n",
        "# Load the data to find the date range\n",
        "data = pd.read_parquet(FILE_PATH)\n",
        "data['time'] = pd.to_datetime(data['time'])\n",
        "\n",
        "# Determine the start and end dates of the test set\n",
        "test_set_start = data[data['time'] > VALIDATION_CUTOFF_DATE]['time'].min()\n",
        "test_set_end = data[data['time'] > VALIDATION_CUTOFF_DATE]['time'].max()\n",
        "\n",
        "print(f\"Test set starts on: {test_set_start}\")\n",
        "print(f\"Test set ends on:   {test_set_end}\")"
      ],
      "metadata": {
        "id": "GPJY_BMhYGfu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}